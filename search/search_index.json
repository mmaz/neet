{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MIT NEET Machine Learning Labs Spring 2019 Imitation Learning Lab Reinforcement Learning Lab NEET Server Instructions","title":"MIT NEET Machine Learning Labs"},{"location":"#mit-neet-machine-learning-labs","text":"","title":"MIT NEET Machine Learning Labs"},{"location":"#spring-2019","text":"Imitation Learning Lab Reinforcement Learning Lab NEET Server Instructions","title":"Spring 2019"},{"location":"imitation_learning/","text":"Imitation Learning Lab Introduction This lab provides an introduction to end-to-end imitation learning for vision-only navigation of a racetrack. Let's break that down: We will train a deep learning model - specifically, a convolutional neural network (CNN) - to regress a steering angle directly from an image taken from the \"front bumper\" of a car. Here, \"imitation learning\" refers to a branch of supervised machine learning which focuses on imitating behavior from human-provided examples. In our case, we will drive a car around a track several times to provide examples for the CNN to mimic. This learning objective is also frequently termed behavioral cloning. We will contrast this with our next lab on \"reinforcement learning\" where a robot agent learns to accomplish a goal via exploration , not via examples. \"Vision-only\" refers to using an RGB camera as the only input to the machine learning algorithm. LIDAR, depth, or vehicle IMU data are not used. Here, \"end-to-end learning\" is shorthand for the CNN's ability to regress a steering angle (i.e., an actuation for the Ackermann steering controller) from unprocessed input data (pixels). We will not need to pre-process input features ourselves, such as extracting corners, walls, floors, or optical flow data. The CNN will learn which features are important, and perform all the steps from image processing to control estimation itself (\"end-to-end\", loosely speaking). We will start by driving a simulated car around a virtual racetrack and collecting camera data from the rendered game engine, as well as our game inputs. We will define a CNN that will regress similar game inputs in order for the car to complete the same track autonomously. Next, we will train the model using camera data and steering angles collected from the RACECAR platform in a real-world environment, the basement in Stata Center, in order for the RACECAR to autonomously drive through the Stata basement. In simulation: Lake Track Jungle Track In Stata basement: This lab and the CNN architecture we will use are based on PilotNet from Nvidia: Nvidia's blog post introducing the concept and their results Nvidia's PilotNet paper Udacity's Unity3D-based Self-Driving-Car Simulator and Naoki Shibuya's drive.py contributions Part 1: Install required Python libraries and the simulation environment Heads up! If you are using an account on the NEET server, skip this step! These dependencies are already installed. TensorFlow, a deep-learning framework You first need to install miniconda to install TensorFlow. Download the Python 3.7 version of miniconda and follow the installation instructions for your platform. Note Even though you will be installing miniconda-python-3.7 , we will be using Python 2.7 to define and train the PilotNet CNN model. miniconda-python-3.7 will handle creating a Python 2.7 environment for you. Once we save a trained model (also known as saving weights ), we can later import the saved model in a Python 2.7 ROS environment on the RACECAR. (To note for completeness, is also possible to train a model with Python 3 and import it with Python 2) Once you have installed miniconda, clone the following repository locally: $ git clone https://github.com/mmaz/imitation_learning_lab $ cd imitation_learning_lab/ Next, we will install TensorFlow using the conda command. There are two options: If you do not have a GPU on your computer: # Use TensorFlow without a GPU $ conda env create -f environment.yml Otherwise, if you do have a GPU: # Use TensorFlow with a GPU $ conda env create -f environment-gpu.yml Udacity self-driving-car simulator Download the Udacity Term 1 simulator for your platform: Linux Mac Windows The full Unity3D source code of this simulator is available here , as well as other simulators for LIDAR data, waypoint-following, traffic, etc. We will only be using the simulator linked above. Extract the simulator (which will create a folder called beta_simulator_linux/ ): $ unzip term1-simulator-linux.zip On Linux, you will need to make the simulator executable, via the chmod command: $ chmod +x ./beta_simulator_linux/beta_simulator.x86_64 Part 2: Defining the PilotNet model Let us take a closer look at the CNN architecture for PilotNet: In this lab, we will command a fixed driving velocity and only regress steering angles from images using PilotNet. Hence, the PilotNet CNN has a single output. Using TensorFlow's Keras API , let us look at an implementation of the above network in code: from tensorflow.keras.layers import Lambda, Conv2D, MaxPooling2D, Dropout, Dense, Flatten from tensorflow.keras.models import Sequential # you will need to crop or shrink images to the dimensions you choose here: IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS = 66, 200, 3 INPUT_SHAPE = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS) def build_model(dropout_rate=0.5): model = Sequential() model.add(Lambda(lambda x: x/127.5-1.0, input_shape=INPUT_SHAPE)) #normalizes image data model.add(Conv2D(24, (5,5), strides=(2, 2), activation='elu')) model.add(Conv2D(36, (5,5), strides=(2, 2), activation='elu')) model.add(Conv2D(48, (5,5), strides=(2, 2), activation='elu')) model.add(Conv2D(64, (3,3), activation='elu')) model.add(Conv2D(64, (3,3), activation='elu')) model.add(Dropout(dropout_rate)) model.add(Flatten()) model.add(Dense(100, activation='elu')) model.add(Dense(50, activation='elu')) model.add(Dense(10, activation='elu')) model.add(Dense(1)) model.summary() return model As configured above, the PilotNet CNN model expects 200x66 crops from the car's camera. Exercise How many trainable parameters are in this model? What is the output volume of each layer? What is the effect of changing the input size on the total number of parameters in the model? Hint 1: use model.summary() to print out a summary of the network. Hint 2: Consider the input to the flattening operation and first dense layer: it is the output volume from the last convolutional layer. How is this affected by changing the input size? What about the next dense layer? For more on TensorFlow's Keras API, click here . Note Note that Keras will disable Dropout regularization at inference time. See here for details. Model Output and Optimization The output of this model is a single neuron, which corresponds to the servo or steering angle to command the car with. In the section on Training we will normalize the steering angle training data to fit between (-1, 1), and therefore we should expect regressions from the CNN to also fit between this range. Question Notice that we also normalize the input images in the first layer between (-1,1) Why would we prefer to normalize the input and output data between these ranges? Hint: Consider the shape, domain, and range of common activation functions. We will use the Adam optimizer with a loss function (i.e., cost or objective function) that minimizes the mean square error betwen the ground-truth steering angles and the currently predicted steering angles: model = build_model() model.compile(loss='mean_squared_error', optimizer=Adam(lr=1.0e-4)) Optional Exercise With only a few changes to the above model and loss definitions, you can add a second output to estimate the velocity as well. This might help your team to complete the course faster! For instance, when you are collecting training data, you might want to drive quickly down straight hallways and slow down during turns. It is feasible to learn this behavior! Part 3: Training the Model We will use three cameras mounted on the simulated car and the real-world RACECAR to collect training data. This excerpt from Nvidia's blog post explains why doing so is useful: Training data contains single images sampled from the video, paired with the corresponding steering command (1/r). Training with data from only the human driver is not sufficient; the network must also learn how to recover from any mistakes, or the car will slowly drift off the road. The training data is therefore augmented with additional images that show the car in different shifts from the center of the lane and rotations from the direction of the road. The images for two specific off-center shifts can be obtained from the left and the right cameras. Additional shifts between the cameras and all rotations are simulated through viewpoint transformation of the image from the nearest camera. Precise viewpoint transformation requires 3D scene knowledge which we don\u2019t have, so we approximate the transformation by assuming all points below the horizon are on flat ground, and all points above the horizon are infinitely far away. This works fine for flat terrain Here is a diagram from Nvidia that describes the training and data augmentation process for PilotNet: Jupyter Notebook We will train our models in Jupyter Notebook: $ conda activate imitation_learning (imitation_learning) $ cd imitation_learning_lab (imitation_learning) $ jupyter notebook Then, open train_RACECAR_pilotnet.ipynb in your browser. In Simulation First, create a new folder to store training data from the simulator, e.g. training/ ) and then start the simulator. On linux: $ ./beta_simulator_linux/beta_simulator.x86_64 Now launch the Training mode and configure the simulator to save data to the folder you created: Once you have configured a folder to record your training data into, press record again (or r as a shortcut) and start to drive the car around (you can use WASD or your arrow keys on your keyboard). If this is your first time running the simulator, stop recording after a few seconds, to inspect the saved results. The simulator will save three camera views from the car: left, center, and right camera views from the bumper of the car (as jpgs) along with the image filenames and the current steering angle in driving_log.csv . Warning Recording will generate a lot of files! Too many files in a single directory can cause performance issues, e.g., when generating thumbnails. Once you are ready to record full laps of the course, I recommend keeping each recording session to a few laps of the track, and making multiple new folders. This will help to keep the number of files within each folder low, e.g., two_laps_run1/ , two_laps_run2/ , etc, or making a folder for tricky sections of the course, e.g., bridge_section_run1/ . It is easy to concatenate the resulting CSVs in python (using simple list concatenation with + ) In the training/ folder (or whichever folder you just created), you should see driving_log.csv and another folder IMG/ . driving_log.csv contains path locations for the three camera views with associated timestamps (sequentially increasing), and the saved steering angle (without a CSV header): Center Left Right Steering Angle Throttle Brake Speed center_2019_03_11_12_22_15_385.jpg left_2019_03_11_12_22_15_385.jpg right_2019_03_11_12_22_15_385.jpg 0 0 0 0 center_2019_03_11_12_22_15_502.jpg left_2019_03_11_12_22_15_502.jpg right_2019_03_11_12_22_15_502.jpg 0 0 0 0 center_2019_03_11_12_22_15_594.jpg left_2019_03_11_12_22_15_594.jpg right_2019_03_11_12_22_15_594.jpg 0 0 0 0 Note If you would like to change some of the parameters, such as the saved image resolution, or even define a new track, you can rebuild and edit the simulator using Unity3D . See this section of the Udacity source code if you are curious how the image files and CSV data are generated. In the training/IMG/ folder you will find .jpg files with the following naming scheme corresponding to the above CSV: Left Center Right center_2019_03_11_12_22_15_385.jpg left_2019_03_11_12_22_15_385.jpg right_2019_03_11_12_22_15_385.jpg Train/Validation Split Regularization With enough training time and enough model parameters, you can perfectly fit your training data! This is called overfitting - we will use validation data, image augmentation, and regularization to avoid overfitting. We will partition our data into training and validation sets. Validation helps to ensure your model is not overfitting on the training data. In the notebook, observe the use of from sklearn.model_selection import train_test_split. imgs = [] angles_rad = [] #normalize between -pi to pi or -1 to 1 with open(driving_data) as fh: ########################## # ### TODO: read in the CSV # ### and fill in the above # ### lists # ############################ TEST_SIZE_FRACTION = 0.2 SEED = 56709 # a fixed seed can be convenient for later comparisons X_train, X_valid, y_train, y_valid = train_test_split( imgs, angles_rad, test_size=TEST_SIZE_FRACTION, random_state=SEED) Batch Generation, Checkpointing, and Training Execution For efficient training on a GPU, multiple examples are sent at once in a batch onto the GPU in a single copy operation, and the results of backpropagation are returned from the GPU back to the CPU. You will want to checkpoint your model after each epoch of training. Lastly, model.fit_generator() will commence training on your data and display the current loss on your training and testing data: checkpoint = ModelCheckpoint('imitationlearning-{epoch:03d}.h5', monitor='val_loss', verbose=0, save_best_only=False, mode='auto') def batch_generator(image_paths, steering_angles, batch_size): \"\"\" Generate training image give image paths and associated steering angles \"\"\" images = np.empty([batch_size, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS]) steers = np.empty(batch_size) while True: i = 0 for index in np.random.permutation(len(image_paths)): ############################################## # TODO: add your augmentation code here! #### # NOTE: you may want to disable #### # augmentation when validating! #### ############################################## image = cv.imread(image_paths[index]) cropped = image[95:-95, 128:-127, :] images[i] = cropped steering_angle = steering_angles[index] steers[i] = steering_angle i += 1 if i == batch_size: break yield images, steers BATCH_SIZE=20 model.fit_generator(generator=batch_generator(X_train, y_train, batch_size=BATCH_SIZE), steps_per_epoch=20000, epochs=10, validation_data=batch_generator(X_valid, y_valid, batch_size=BATCH_SIZE), # https://stackoverflow.com/a/45944225 validation_steps=len(X_valid) // BATCH_SIZE, callbacks=[checkpoint], verbose=1) Image Augmentation You will want to add some data augmentation to help your model generalize past the specific examples you have collected in the simulator (and on the actual RACECAR). Some example transformations to incorporate: Center Image Left and right Images def choose_image(data_dir, center, left, right, steering_angle): \"\"\" Randomly choose an image from the center, left or right, and adjust the steering angle. \"\"\" choice = np.random.choice(3) if choice == 0: return load_image(data_dir, left), steering_angle + 0.2 elif choice == 1: return load_image(data_dir, right), steering_angle - 0.2 return load_image(data_dir, center), steering_angle Flipped Image if np.random.rand() < 0.5: image = cv2.flip(image, 1) steering_angle = -steering_angle return image, steering_angle Translated Image def random_translate(image, steering_angle, range_x, range_y): \"\"\" Randomly shift the image virtially and horizontally (translation). \"\"\" trans_x = range_x * (np.random.rand() - 0.5) trans_y = range_y * (np.random.rand() - 0.5) steering_angle += trans_x * 0.002 trans_m = np.float32([[1, 0, trans_x], [0, 1, trans_y]]) height, width = image.shape[:2] image = cv2.warpAffine(image, trans_m, (width, height)) return image, steering_angle Servo histograms It is important to ensure the train/test split of the data you have collected have similar driving condition represented. For instance, here is the histogram of servo angles in the training and testing data used above: Checkpointing checkpoint = ModelCheckpoint('model-{epoch:03d}.h5', monitor='val_loss', verbose=0, save_best_only=False, mode='auto') [Optional] Extending to more general environments It is possible to train a model with driving data from public roads, in order to experiment with how it affects the performance of your car in Stata basement. Danger Obviously, you should not test anything on public roads yourself, either on a RACECAR or any other car. Be safe and responsible! You can find useful public road data from Udacity here: https://github.com/udacity/self-driving-car/tree/master/datasets Another useful public road dataset is here: https://github.com/SullyChen/driving-datasets Part 4: RACECAR data collection and training In this section you will manually collect steering angle data by driving the car around. A good first task is to train the RACECAR to drive around some tables in a circle, before tackling Stata basement. You can also define some more intermediate-difficulty courses (figure-eights, snake patterns, etc) to gain intuition on what types of training and validation methods are most effective. Here is an example of training data collected around some tables in a classroom: And here is a third-person view of a car autonomously driving around the same path using PilotNet: The following script will record images from the three webcams on the RACECAR along with the joystick-commanded steering angle (through teleop ): https://github.com/mmaz/imitation_learning_lab/blob/master/record_RACECAR.py $ python2 record_RACECAR.py Note Make sure to use python2 here for recording the images and steering angle data - note that we will not use python2 in the next section to access the cameras. TensorFlow is only available in the car's python3 environment, and ROS is only available in our python2 environment. For recording, we do not need access to TensorFlow, only OpenCV. For autonomous driving with both TensorFlow and ROS, we will see how to work around this inconvenience in the next section via zmq . Note that you will need to change the Video Device IDs to the appropriate values, depending on which order the webcams were plugged in and registered by Linux. Set the appropriate values for your car in camera_RACECAR.py class Video: # dev/video* LEFT = 1 CENTER = 2 RIGHT = 0 The following script will display the currently assigned video device IDs on top of the camera feeds, to help verify the IDs are in the correct order: $ python3 video_id_RACECAR.py python2 should also work above. Sidenote: you can also set udev rules to \"freeze\" these values for your car, if you frequently find the IDs changing after power-cycling the RACECAR. After you have collected your training data, transfer the data using scp or a flashdrive to your laptop and train your model using the provided jupyter notebook . Reminder You should not train a model on the RACECAR - use the course server or your own laptop! Part 5: Running inference on RACECAR To execute a trained model, you will need to run the following scripts: https://github.com/mmaz/imitation_learning_lab/blob/master/infer_RACECAR.py https://github.com/mmaz/imitation_learning_lab/blob/master/drive_RACECAR.py Note We use zmq to send steering angle messages from our Python3 script running inference with TensorFlow, over to a Python2-ROS script that commands the car to drive. You will first need to copy your saved model weights to the RACECAR (e.g., using SCP). You will specify the model location using this command-line argument . Next, if it has changed (due to a reboot or unplugging the cameras), remember to modify the video ID to the center camera here , or verify the current ID is correct using video_id_RACECAR.py : class Video: # dev/video* LEFT = 1 CENTER = 2 RIGHT = 0 Note We only need the center camera during inference. Then, in one terminal, run: $ python3 infer_RACECAR.py --model path_to_model.h5 Ensure you are using python3 above. In another terminal, use python2 and run: $ python2 drive_RACECAR.py Optional exercise This script also includes a mean filter. You can remove this, extend or shorten the length of the mean filter, change it to a median filter, etc, to experiment with inference behavior while driving. visualize_drive.ipynb can be used to overlay steering angle predictions on top of saved runs (see infer_RACECAR.py for a flag that saves images during inference):","title":"Imitation Learning Lab"},{"location":"imitation_learning/#imitation-learning-lab","text":"","title":"Imitation Learning Lab"},{"location":"imitation_learning/#introduction","text":"This lab provides an introduction to end-to-end imitation learning for vision-only navigation of a racetrack. Let's break that down: We will train a deep learning model - specifically, a convolutional neural network (CNN) - to regress a steering angle directly from an image taken from the \"front bumper\" of a car. Here, \"imitation learning\" refers to a branch of supervised machine learning which focuses on imitating behavior from human-provided examples. In our case, we will drive a car around a track several times to provide examples for the CNN to mimic. This learning objective is also frequently termed behavioral cloning. We will contrast this with our next lab on \"reinforcement learning\" where a robot agent learns to accomplish a goal via exploration , not via examples. \"Vision-only\" refers to using an RGB camera as the only input to the machine learning algorithm. LIDAR, depth, or vehicle IMU data are not used. Here, \"end-to-end learning\" is shorthand for the CNN's ability to regress a steering angle (i.e., an actuation for the Ackermann steering controller) from unprocessed input data (pixels). We will not need to pre-process input features ourselves, such as extracting corners, walls, floors, or optical flow data. The CNN will learn which features are important, and perform all the steps from image processing to control estimation itself (\"end-to-end\", loosely speaking). We will start by driving a simulated car around a virtual racetrack and collecting camera data from the rendered game engine, as well as our game inputs. We will define a CNN that will regress similar game inputs in order for the car to complete the same track autonomously. Next, we will train the model using camera data and steering angles collected from the RACECAR platform in a real-world environment, the basement in Stata Center, in order for the RACECAR to autonomously drive through the Stata basement.","title":"Introduction"},{"location":"imitation_learning/#in-simulation","text":"Lake Track Jungle Track","title":"In simulation:"},{"location":"imitation_learning/#in-stata-basement","text":"This lab and the CNN architecture we will use are based on PilotNet from Nvidia: Nvidia's blog post introducing the concept and their results Nvidia's PilotNet paper Udacity's Unity3D-based Self-Driving-Car Simulator and Naoki Shibuya's drive.py contributions","title":"In Stata basement:"},{"location":"imitation_learning/#part-1-install-required-python-libraries-and-the-simulation-environment","text":"Heads up! If you are using an account on the NEET server, skip this step! These dependencies are already installed.","title":"Part 1: Install required Python libraries and the simulation environment"},{"location":"imitation_learning/#tensorflow-a-deep-learning-framework","text":"You first need to install miniconda to install TensorFlow. Download the Python 3.7 version of miniconda and follow the installation instructions for your platform. Note Even though you will be installing miniconda-python-3.7 , we will be using Python 2.7 to define and train the PilotNet CNN model. miniconda-python-3.7 will handle creating a Python 2.7 environment for you. Once we save a trained model (also known as saving weights ), we can later import the saved model in a Python 2.7 ROS environment on the RACECAR. (To note for completeness, is also possible to train a model with Python 3 and import it with Python 2) Once you have installed miniconda, clone the following repository locally: $ git clone https://github.com/mmaz/imitation_learning_lab $ cd imitation_learning_lab/ Next, we will install TensorFlow using the conda command. There are two options: If you do not have a GPU on your computer: # Use TensorFlow without a GPU $ conda env create -f environment.yml Otherwise, if you do have a GPU: # Use TensorFlow with a GPU $ conda env create -f environment-gpu.yml","title":"TensorFlow, a deep-learning framework"},{"location":"imitation_learning/#udacity-self-driving-car-simulator","text":"Download the Udacity Term 1 simulator for your platform: Linux Mac Windows The full Unity3D source code of this simulator is available here , as well as other simulators for LIDAR data, waypoint-following, traffic, etc. We will only be using the simulator linked above. Extract the simulator (which will create a folder called beta_simulator_linux/ ): $ unzip term1-simulator-linux.zip On Linux, you will need to make the simulator executable, via the chmod command: $ chmod +x ./beta_simulator_linux/beta_simulator.x86_64","title":"Udacity self-driving-car simulator"},{"location":"imitation_learning/#part-2-defining-the-pilotnet-model","text":"Let us take a closer look at the CNN architecture for PilotNet: In this lab, we will command a fixed driving velocity and only regress steering angles from images using PilotNet. Hence, the PilotNet CNN has a single output. Using TensorFlow's Keras API , let us look at an implementation of the above network in code: from tensorflow.keras.layers import Lambda, Conv2D, MaxPooling2D, Dropout, Dense, Flatten from tensorflow.keras.models import Sequential # you will need to crop or shrink images to the dimensions you choose here: IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS = 66, 200, 3 INPUT_SHAPE = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS) def build_model(dropout_rate=0.5): model = Sequential() model.add(Lambda(lambda x: x/127.5-1.0, input_shape=INPUT_SHAPE)) #normalizes image data model.add(Conv2D(24, (5,5), strides=(2, 2), activation='elu')) model.add(Conv2D(36, (5,5), strides=(2, 2), activation='elu')) model.add(Conv2D(48, (5,5), strides=(2, 2), activation='elu')) model.add(Conv2D(64, (3,3), activation='elu')) model.add(Conv2D(64, (3,3), activation='elu')) model.add(Dropout(dropout_rate)) model.add(Flatten()) model.add(Dense(100, activation='elu')) model.add(Dense(50, activation='elu')) model.add(Dense(10, activation='elu')) model.add(Dense(1)) model.summary() return model As configured above, the PilotNet CNN model expects 200x66 crops from the car's camera. Exercise How many trainable parameters are in this model? What is the output volume of each layer? What is the effect of changing the input size on the total number of parameters in the model? Hint 1: use model.summary() to print out a summary of the network. Hint 2: Consider the input to the flattening operation and first dense layer: it is the output volume from the last convolutional layer. How is this affected by changing the input size? What about the next dense layer? For more on TensorFlow's Keras API, click here . Note Note that Keras will disable Dropout regularization at inference time. See here for details.","title":"Part 2: Defining the PilotNet model"},{"location":"imitation_learning/#model-output-and-optimization","text":"The output of this model is a single neuron, which corresponds to the servo or steering angle to command the car with. In the section on Training we will normalize the steering angle training data to fit between (-1, 1), and therefore we should expect regressions from the CNN to also fit between this range. Question Notice that we also normalize the input images in the first layer between (-1,1) Why would we prefer to normalize the input and output data between these ranges? Hint: Consider the shape, domain, and range of common activation functions. We will use the Adam optimizer with a loss function (i.e., cost or objective function) that minimizes the mean square error betwen the ground-truth steering angles and the currently predicted steering angles: model = build_model() model.compile(loss='mean_squared_error', optimizer=Adam(lr=1.0e-4)) Optional Exercise With only a few changes to the above model and loss definitions, you can add a second output to estimate the velocity as well. This might help your team to complete the course faster! For instance, when you are collecting training data, you might want to drive quickly down straight hallways and slow down during turns. It is feasible to learn this behavior!","title":"Model Output and Optimization"},{"location":"imitation_learning/#part-3-training-the-model","text":"We will use three cameras mounted on the simulated car and the real-world RACECAR to collect training data. This excerpt from Nvidia's blog post explains why doing so is useful: Training data contains single images sampled from the video, paired with the corresponding steering command (1/r). Training with data from only the human driver is not sufficient; the network must also learn how to recover from any mistakes, or the car will slowly drift off the road. The training data is therefore augmented with additional images that show the car in different shifts from the center of the lane and rotations from the direction of the road. The images for two specific off-center shifts can be obtained from the left and the right cameras. Additional shifts between the cameras and all rotations are simulated through viewpoint transformation of the image from the nearest camera. Precise viewpoint transformation requires 3D scene knowledge which we don\u2019t have, so we approximate the transformation by assuming all points below the horizon are on flat ground, and all points above the horizon are infinitely far away. This works fine for flat terrain Here is a diagram from Nvidia that describes the training and data augmentation process for PilotNet:","title":"Part 3: Training the Model"},{"location":"imitation_learning/#jupyter-notebook","text":"We will train our models in Jupyter Notebook: $ conda activate imitation_learning (imitation_learning) $ cd imitation_learning_lab (imitation_learning) $ jupyter notebook Then, open train_RACECAR_pilotnet.ipynb in your browser.","title":"Jupyter Notebook"},{"location":"imitation_learning/#in-simulation_1","text":"First, create a new folder to store training data from the simulator, e.g. training/ ) and then start the simulator. On linux: $ ./beta_simulator_linux/beta_simulator.x86_64 Now launch the Training mode and configure the simulator to save data to the folder you created: Once you have configured a folder to record your training data into, press record again (or r as a shortcut) and start to drive the car around (you can use WASD or your arrow keys on your keyboard). If this is your first time running the simulator, stop recording after a few seconds, to inspect the saved results. The simulator will save three camera views from the car: left, center, and right camera views from the bumper of the car (as jpgs) along with the image filenames and the current steering angle in driving_log.csv . Warning Recording will generate a lot of files! Too many files in a single directory can cause performance issues, e.g., when generating thumbnails. Once you are ready to record full laps of the course, I recommend keeping each recording session to a few laps of the track, and making multiple new folders. This will help to keep the number of files within each folder low, e.g., two_laps_run1/ , two_laps_run2/ , etc, or making a folder for tricky sections of the course, e.g., bridge_section_run1/ . It is easy to concatenate the resulting CSVs in python (using simple list concatenation with + ) In the training/ folder (or whichever folder you just created), you should see driving_log.csv and another folder IMG/ . driving_log.csv contains path locations for the three camera views with associated timestamps (sequentially increasing), and the saved steering angle (without a CSV header): Center Left Right Steering Angle Throttle Brake Speed center_2019_03_11_12_22_15_385.jpg left_2019_03_11_12_22_15_385.jpg right_2019_03_11_12_22_15_385.jpg 0 0 0 0 center_2019_03_11_12_22_15_502.jpg left_2019_03_11_12_22_15_502.jpg right_2019_03_11_12_22_15_502.jpg 0 0 0 0 center_2019_03_11_12_22_15_594.jpg left_2019_03_11_12_22_15_594.jpg right_2019_03_11_12_22_15_594.jpg 0 0 0 0 Note If you would like to change some of the parameters, such as the saved image resolution, or even define a new track, you can rebuild and edit the simulator using Unity3D . See this section of the Udacity source code if you are curious how the image files and CSV data are generated. In the training/IMG/ folder you will find .jpg files with the following naming scheme corresponding to the above CSV: Left Center Right center_2019_03_11_12_22_15_385.jpg left_2019_03_11_12_22_15_385.jpg right_2019_03_11_12_22_15_385.jpg","title":"In Simulation"},{"location":"imitation_learning/#trainvalidation-split","text":"Regularization With enough training time and enough model parameters, you can perfectly fit your training data! This is called overfitting - we will use validation data, image augmentation, and regularization to avoid overfitting. We will partition our data into training and validation sets. Validation helps to ensure your model is not overfitting on the training data. In the notebook, observe the use of from sklearn.model_selection import train_test_split. imgs = [] angles_rad = [] #normalize between -pi to pi or -1 to 1 with open(driving_data) as fh: ########################## # ### TODO: read in the CSV # ### and fill in the above # ### lists # ############################ TEST_SIZE_FRACTION = 0.2 SEED = 56709 # a fixed seed can be convenient for later comparisons X_train, X_valid, y_train, y_valid = train_test_split( imgs, angles_rad, test_size=TEST_SIZE_FRACTION, random_state=SEED)","title":"Train/Validation Split"},{"location":"imitation_learning/#batch-generation-checkpointing-and-training-execution","text":"For efficient training on a GPU, multiple examples are sent at once in a batch onto the GPU in a single copy operation, and the results of backpropagation are returned from the GPU back to the CPU. You will want to checkpoint your model after each epoch of training. Lastly, model.fit_generator() will commence training on your data and display the current loss on your training and testing data: checkpoint = ModelCheckpoint('imitationlearning-{epoch:03d}.h5', monitor='val_loss', verbose=0, save_best_only=False, mode='auto') def batch_generator(image_paths, steering_angles, batch_size): \"\"\" Generate training image give image paths and associated steering angles \"\"\" images = np.empty([batch_size, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS]) steers = np.empty(batch_size) while True: i = 0 for index in np.random.permutation(len(image_paths)): ############################################## # TODO: add your augmentation code here! #### # NOTE: you may want to disable #### # augmentation when validating! #### ############################################## image = cv.imread(image_paths[index]) cropped = image[95:-95, 128:-127, :] images[i] = cropped steering_angle = steering_angles[index] steers[i] = steering_angle i += 1 if i == batch_size: break yield images, steers BATCH_SIZE=20 model.fit_generator(generator=batch_generator(X_train, y_train, batch_size=BATCH_SIZE), steps_per_epoch=20000, epochs=10, validation_data=batch_generator(X_valid, y_valid, batch_size=BATCH_SIZE), # https://stackoverflow.com/a/45944225 validation_steps=len(X_valid) // BATCH_SIZE, callbacks=[checkpoint], verbose=1)","title":"Batch Generation, Checkpointing, and Training Execution"},{"location":"imitation_learning/#image-augmentation","text":"You will want to add some data augmentation to help your model generalize past the specific examples you have collected in the simulator (and on the actual RACECAR). Some example transformations to incorporate: Center Image Left and right Images def choose_image(data_dir, center, left, right, steering_angle): \"\"\" Randomly choose an image from the center, left or right, and adjust the steering angle. \"\"\" choice = np.random.choice(3) if choice == 0: return load_image(data_dir, left), steering_angle + 0.2 elif choice == 1: return load_image(data_dir, right), steering_angle - 0.2 return load_image(data_dir, center), steering_angle Flipped Image if np.random.rand() < 0.5: image = cv2.flip(image, 1) steering_angle = -steering_angle return image, steering_angle Translated Image def random_translate(image, steering_angle, range_x, range_y): \"\"\" Randomly shift the image virtially and horizontally (translation). \"\"\" trans_x = range_x * (np.random.rand() - 0.5) trans_y = range_y * (np.random.rand() - 0.5) steering_angle += trans_x * 0.002 trans_m = np.float32([[1, 0, trans_x], [0, 1, trans_y]]) height, width = image.shape[:2] image = cv2.warpAffine(image, trans_m, (width, height)) return image, steering_angle","title":"Image Augmentation"},{"location":"imitation_learning/#servo-histograms","text":"It is important to ensure the train/test split of the data you have collected have similar driving condition represented. For instance, here is the histogram of servo angles in the training and testing data used above:","title":"Servo histograms"},{"location":"imitation_learning/#checkpointing","text":"checkpoint = ModelCheckpoint('model-{epoch:03d}.h5', monitor='val_loss', verbose=0, save_best_only=False, mode='auto')","title":"Checkpointing"},{"location":"imitation_learning/#optional-extending-to-more-general-environments","text":"It is possible to train a model with driving data from public roads, in order to experiment with how it affects the performance of your car in Stata basement. Danger Obviously, you should not test anything on public roads yourself, either on a RACECAR or any other car. Be safe and responsible! You can find useful public road data from Udacity here: https://github.com/udacity/self-driving-car/tree/master/datasets Another useful public road dataset is here: https://github.com/SullyChen/driving-datasets","title":"[Optional] Extending to more general environments"},{"location":"imitation_learning/#part-4-racecar-data-collection-and-training","text":"In this section you will manually collect steering angle data by driving the car around. A good first task is to train the RACECAR to drive around some tables in a circle, before tackling Stata basement. You can also define some more intermediate-difficulty courses (figure-eights, snake patterns, etc) to gain intuition on what types of training and validation methods are most effective. Here is an example of training data collected around some tables in a classroom: And here is a third-person view of a car autonomously driving around the same path using PilotNet: The following script will record images from the three webcams on the RACECAR along with the joystick-commanded steering angle (through teleop ): https://github.com/mmaz/imitation_learning_lab/blob/master/record_RACECAR.py $ python2 record_RACECAR.py Note Make sure to use python2 here for recording the images and steering angle data - note that we will not use python2 in the next section to access the cameras. TensorFlow is only available in the car's python3 environment, and ROS is only available in our python2 environment. For recording, we do not need access to TensorFlow, only OpenCV. For autonomous driving with both TensorFlow and ROS, we will see how to work around this inconvenience in the next section via zmq . Note that you will need to change the Video Device IDs to the appropriate values, depending on which order the webcams were plugged in and registered by Linux. Set the appropriate values for your car in camera_RACECAR.py class Video: # dev/video* LEFT = 1 CENTER = 2 RIGHT = 0 The following script will display the currently assigned video device IDs on top of the camera feeds, to help verify the IDs are in the correct order: $ python3 video_id_RACECAR.py python2 should also work above. Sidenote: you can also set udev rules to \"freeze\" these values for your car, if you frequently find the IDs changing after power-cycling the RACECAR. After you have collected your training data, transfer the data using scp or a flashdrive to your laptop and train your model using the provided jupyter notebook . Reminder You should not train a model on the RACECAR - use the course server or your own laptop!","title":"Part 4: RACECAR data collection and training"},{"location":"imitation_learning/#part-5-running-inference-on-racecar","text":"To execute a trained model, you will need to run the following scripts: https://github.com/mmaz/imitation_learning_lab/blob/master/infer_RACECAR.py https://github.com/mmaz/imitation_learning_lab/blob/master/drive_RACECAR.py Note We use zmq to send steering angle messages from our Python3 script running inference with TensorFlow, over to a Python2-ROS script that commands the car to drive. You will first need to copy your saved model weights to the RACECAR (e.g., using SCP). You will specify the model location using this command-line argument . Next, if it has changed (due to a reboot or unplugging the cameras), remember to modify the video ID to the center camera here , or verify the current ID is correct using video_id_RACECAR.py : class Video: # dev/video* LEFT = 1 CENTER = 2 RIGHT = 0 Note We only need the center camera during inference. Then, in one terminal, run: $ python3 infer_RACECAR.py --model path_to_model.h5 Ensure you are using python3 above. In another terminal, use python2 and run: $ python2 drive_RACECAR.py Optional exercise This script also includes a mean filter. You can remove this, extend or shorten the length of the mean filter, change it to a median filter, etc, to experiment with inference behavior while driving. visualize_drive.ipynb can be used to overlay steering angle predictions on top of saved runs (see infer_RACECAR.py for a flag that saves images during inference):","title":"Part 5: Running inference on RACECAR"},{"location":"neet_fall_2018/","text":"MIT NEET Fall 2018 Multiplex SSH This is highly recommended for connecting to the drones: Create or edit a file in the .ssh/ directory inside your home directory, called config : $ nano ~/.ssh/config or $ gedit ~/.ssh/config or $ vi ~/.ssh/config (or whichever text editor you are comfortable with) You will need to add two lines to the file, and you will need to replace PATH_TO_HOME with the correct path for your platform: on linux, /home/YOURTEAMNAME/.ssh/ (or your username, if you are not using a team laptop) on mac, /Users/YOURUSERNAME/.ssh windows, (TODO) Add the following two lines ( Remember: change PATH_TO_HOME as specified above): ControlMaster auto ControlPath PATH_TO_HOME/.ssh/ssh_mux_%h_%p_%r Important - remember to start your FIRST SSH connection with -Y if you plan to use xforwarding (e.g., for rqt_image_view ) Directory Setup These instructions replace this section on the website. On the SSH window on your team laptop, enter the following commands cd ~ cd ~/bwsi-uav/catkin_ws/src git clone https://github.com/BWSI-UAV/aero_control.git cd aero_control git remote add upstream https://github.com/BWSI-UAV/aero_control.git cd ~/bwsi-uav git clone https://github.com/BWSI-UAV/laboratory.git cd laboratory git remote add upstream https://github.com/BWSI-UAV/laboratory.git cd ~/bwsi-uav git clone https://github.com/BWSI-UAV/documents.git cd documents git remote add upstream https://github.com/BWSI-UAV/documents.git Compressed Camera Feeds To find if your drone supports compressed camera feeds: Start roscore Start optical flow: sudo -E ~/bwsi-uav/catkin-ws/src/aero-optical-flow/build/aero-optical-flow $ rostopic list | grep compressed If you don't see /aero_downward_camera/image/compressed in the results you will need to install compressed transport support: sudo apt-get install ros-kinetic-image-transport-plugins then restart your camera feed by restarting the aero-optical-flow binary (step 1 above). To record a compressed downward camera feed: $ cd ~/rosbags/ # or wherever you want to store your rosbag $ time rosbag record -O downward /aero_downward_camera/image/compressed # -O specifies the filename You can then SCP your rosbag to your team laptop. To convert compressed camera messages to OpenCV images, you can't use CVBridge. Here is an OpenCV-specific decoding solution. (You could also use CompressedImage from sensor_msgs.msg ): from __future__ import print_function import cv2 import numpy as np import roslib import rospy from sensor_msgs.msg import CompressedImage # We do not use cv_bridge since it does not support CompressedImage # from cv_bridge import CvBridge, CvBridgeError import rosbag import os DEST = \"/path/to/folder/to/save/images\" BAG = \"/path/to/rosbag.bag\" #your camera topic: CAM = '/aero_downward_camera/image/compressed' def bag2msgs(): bag = rosbag.Bag(BAG) if bag is None: raise ValueError(\"no bag {}\".format(BAG)) msgs = [] for topic, msg, t in bag.read_messages(topics=[CAM]): msgs.append(msg) bag.close() print(\"MESSAGES: {}\".format(len(msgs))) return msgs def uncompress(msgs): imgs = [] for msg in msgs: #### direct conversion to CV2 #### np_arr = np.fromstring(msg.data, np.uint8) image_np = cv2.imdecode(np_arr, cv2.IMREAD_COLOR) # OpenCV >= 3.0: imgs.append(image_np) return imgs if __name__ == \"__main__\": if os.listdir(DEST) != []: raise ValueError('need empty directory for dest {}'.format(DEST)) msgs = bag2msgs() imgs = uncompress(msgs) for idx,im in enumerate(imgs): if idx % 50 == 0: print(idx) imname = \"frame{:05d}.jpg\".format(idx) cv2.imwrite(DEST + imname, im)","title":"MIT NEET Fall 2018"},{"location":"neet_fall_2018/#mit-neet-fall-2018","text":"","title":"MIT NEET Fall 2018"},{"location":"neet_fall_2018/#multiplex-ssh","text":"This is highly recommended for connecting to the drones: Create or edit a file in the .ssh/ directory inside your home directory, called config : $ nano ~/.ssh/config or $ gedit ~/.ssh/config or $ vi ~/.ssh/config (or whichever text editor you are comfortable with) You will need to add two lines to the file, and you will need to replace PATH_TO_HOME with the correct path for your platform: on linux, /home/YOURTEAMNAME/.ssh/ (or your username, if you are not using a team laptop) on mac, /Users/YOURUSERNAME/.ssh windows, (TODO) Add the following two lines ( Remember: change PATH_TO_HOME as specified above): ControlMaster auto ControlPath PATH_TO_HOME/.ssh/ssh_mux_%h_%p_%r Important - remember to start your FIRST SSH connection with -Y if you plan to use xforwarding (e.g., for rqt_image_view )","title":"Multiplex SSH"},{"location":"neet_fall_2018/#directory-setup","text":"These instructions replace this section on the website. On the SSH window on your team laptop, enter the following commands cd ~ cd ~/bwsi-uav/catkin_ws/src git clone https://github.com/BWSI-UAV/aero_control.git cd aero_control git remote add upstream https://github.com/BWSI-UAV/aero_control.git cd ~/bwsi-uav git clone https://github.com/BWSI-UAV/laboratory.git cd laboratory git remote add upstream https://github.com/BWSI-UAV/laboratory.git cd ~/bwsi-uav git clone https://github.com/BWSI-UAV/documents.git cd documents git remote add upstream https://github.com/BWSI-UAV/documents.git","title":"Directory Setup"},{"location":"neet_fall_2018/#compressed-camera-feeds","text":"To find if your drone supports compressed camera feeds: Start roscore Start optical flow: sudo -E ~/bwsi-uav/catkin-ws/src/aero-optical-flow/build/aero-optical-flow $ rostopic list | grep compressed If you don't see /aero_downward_camera/image/compressed in the results you will need to install compressed transport support: sudo apt-get install ros-kinetic-image-transport-plugins then restart your camera feed by restarting the aero-optical-flow binary (step 1 above). To record a compressed downward camera feed: $ cd ~/rosbags/ # or wherever you want to store your rosbag $ time rosbag record -O downward /aero_downward_camera/image/compressed # -O specifies the filename You can then SCP your rosbag to your team laptop. To convert compressed camera messages to OpenCV images, you can't use CVBridge. Here is an OpenCV-specific decoding solution. (You could also use CompressedImage from sensor_msgs.msg ): from __future__ import print_function import cv2 import numpy as np import roslib import rospy from sensor_msgs.msg import CompressedImage # We do not use cv_bridge since it does not support CompressedImage # from cv_bridge import CvBridge, CvBridgeError import rosbag import os DEST = \"/path/to/folder/to/save/images\" BAG = \"/path/to/rosbag.bag\" #your camera topic: CAM = '/aero_downward_camera/image/compressed' def bag2msgs(): bag = rosbag.Bag(BAG) if bag is None: raise ValueError(\"no bag {}\".format(BAG)) msgs = [] for topic, msg, t in bag.read_messages(topics=[CAM]): msgs.append(msg) bag.close() print(\"MESSAGES: {}\".format(len(msgs))) return msgs def uncompress(msgs): imgs = [] for msg in msgs: #### direct conversion to CV2 #### np_arr = np.fromstring(msg.data, np.uint8) image_np = cv2.imdecode(np_arr, cv2.IMREAD_COLOR) # OpenCV >= 3.0: imgs.append(image_np) return imgs if __name__ == \"__main__\": if os.listdir(DEST) != []: raise ValueError('need empty directory for dest {}'.format(DEST)) msgs = bag2msgs() imgs = uncompress(msgs) for idx,im in enumerate(imgs): if idx % 50 == 0: print(idx) imname = \"frame{:05d}.jpg\".format(idx) cv2.imwrite(DEST + imname, im)","title":"Compressed Camera Feeds"},{"location":"reinforcement_learning/","text":"Reinforcement Learning Lab Introduction Objective: This lab exercise introduces deep reinforcement learning (Deep RL) for autonomous driving in simulation, using only a camera for sensing. Reinforcement learning is distinct from imitation learning: here, the robot learns to explore the environment on its own, with practically no prior information about the world or itself. Through exploration and reinforcement of behaviors which net reward, rather than human-provided examples of behavior to imitate, a robot has the potential to learn novel, optimal techniques which exceed the abilities of humans. Atari games, Go, and StarCraft are a few well-known settings in which Deep RL algorithms have approached or surpassed human expertise. This lab relies on providing the robot with a simulation environment to use as a sandbox for exploration. In particular, we will use a Unity-based simulation environment originally developed by Tawn Kramer for the DonkeyCar RC platform . This lab exercise relies on a Deep RL demonstration by Antonin Raffin , which uses Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC) to quickly train the simulated DonkeyCar to drive on a randomly generated track (and also leverages a variational autoencoder based on code from hardmaru ). This demonstration is itself a fork of an earlier repository by Roma Sokolkov , which leveraged another Deep RL algorithm Deep Deterministic Policy Gradient (DDPG) in the simulator. It is instructive to review Antonin Raffin's blogpost regarding the testing he conducted, for ideas and background, as you work on the lab. For the lab exercise we have forked Antonin Raffin's repository in case there are any lab-specific changes to distribute, but the lab fork is otherwise simply tracking the upstream repo by the original authors (Raffin, Sokolov, Kramer, and other contributors/sources ): Cloning the lab locally: $ git clone https://github.com/mmaz/learning-to-drive-in-5-minutes A review of Reinforcement Learning If needed, OpenAI's Spinning Up in Deep RL is an excellent way to review in greater depth the concepts discussed during lecture. In particular, the lab is based on topics covered in these sections: Basic concepts in RL The derivation of policy gradients The \"vanilla PG\" algorithm Proximal Policy Optimization (PPO) Soft Actor-Critic (SAC) Gym Interface \"Gym\" interfaces refer to a de facto-standard for reinforcement learning in simulation, popularized by OpenAI's Gym environment . Simulation environments frequently support variants of the following two calls: reset() step() Calls to step() usually take in actions and return the next state, observed reward (if any), and auxillary information such as whether the episode is over (allowing the RL algorithm time to make decisions and optionally reset() the environment for the next episode). For a more in-depth explanation on the concepts in a Gym API, read http://gym.openai.com/docs/ . State-space Dimensionality Reduction A variational autoencoder or VAE is used to reduce the size of the state space that the policy must consider at training and inference time. The state space is represented as a small vector of floating point numbers (e.g., 20-30) taken from the VAE's bottleneck encoding (i.e., the \"latent space\" encoding), instead of the full image. This lecture from Prof. Ali Ghodsi at the University of Waterloo is an excellent, brief, and self-contained introduction to the theory and implementation of VAEs. For additional reading, please consult one or more of these references (or inquire during office hours/over Slack): Ali Ghodsi's lecture: https://www.youtube.com/watch?v=uaaqyVS9-rM https://jaan.io/what-is-variational-autoencoder-vae-tutorial/ https://www.tensorflow.org/alpha/tutorials/generative/cvae https://blog.keras.io/building-autoencoders-in-keras.html Fast-forward Labs blog Part 1 and Part 2 hardmaru 's series of posts on world models: http://blog.otoro.net//2018/06/09/world-models-experiments/ Part 1: Downloading the DonkeyCar simulation environment The following link is a Linux build of the Unity Donkey simulation environment from the original author: Download on Google Drive Note The simulation can also be built from source for other platforms, from the donkey tree of the sdsandbox repo using the Unity development platform. Starting the simulator After unzipping the folder and changing into the directory, launch the simulator with: $ ./build_sdsandbox.x86_64 I suggest the following settings of 640x480 , windowed , and Fantastic rendering quality (but this wll depend on your graphics support): Simulator implementations of OpenAI Gym functions: The following links to the simulator's Gym API implementation are provided as reference for your experimentation (changing the implementation is not necessary however). Note that editing these implementations will not require rebuilidng the simulator, making experimentation easier to conduct. step() is implemented through several callbacks: take_action() calc_reward() - note that this depends implicitly on the cross-track error reported by the simulator on_telemetry() This recieves data from the simulator, including front-bumper images from the simulated DonkeyCar current steering angle and velocity cross-track error (\"cte\") reset() sets all counters to zero is_game_over() is simply a combination of checking for collisions (not present in level 0) or crossing a threshhold of tolerated cross-track error Part 2: Installing Deep RL python dependencies Heads up! If you are using an account on the NEET server, skip this step! These dependencies are already installed. If you do not have a GPU on your computer: # Use TensorFlow without a GPU $ conda env create -f environment.yml Otherwise, if you do have a GPU: # Use TensorFlow with a GPU $ conda env create -f environment-gpu.yml Part 3: Training a policy with a pre-trained VAE First, download the pre-trained VAE from the author's Google Drive folder for Level 0 in the same directory you cloned the repository into. Next, launch the Unity environment (if it is not already running from Part 1 ) To launch a newly initialized training run for the PPO algorithm across 5000 iterations in Level 0 of the simulator, run the following command: $ python train.py --algo ppo -vae vae-level-0-dim-32.pkl -n 5000 Alternatively, to launch a training run for Soft Actor-Critic (SAC): $ python train.py --algo sac -vae vae-level-0-dim-32.pkl -n 5000 For Level 0 , the simulator will select a random track layout, and the Gym API will be used to reset the simulator using this track layout for each episode when training a new policy. Example Track Layout 1 Example Track Layout 2 Question In addition to accumulated reward, what metrics could be used for measuring a policy's performance? Are 5000 steps enough to train a good policy for PPO or SAC? Across different training runs (each starting from a randomly initialized policy), how frequently will PPO or SAC converge on a good policy, versus suffering from performance collapse? Part 4: Experimenting with Deep RL Once you have tried training a policy in the simulation environment, you can experiment with changing the existing algorithms, or try a different Deep RL algorithm altogether, such as TRPO , TD3 , etc. The goal of this section of the lab is to gain some intuition and experience with training the vehicle's policy using deep reinforcement learning, through modifying the existing code, hyperparameters, and algorithms, or by incorporating new algorithms. Your experimentation can target one or more threads of investigation (this is a non-exhaustive list): How can the training performance (accuracy/speed/etc) and learned policy's effectiveness be visualized or quantified? What is the effect of tuning hyperparameters on the convergence time and robustness (or lack thereof) of algorithms like PPO and SAC? What changes to the algorithm can be made to improve convergence behaviors and the robustness of the learned policy? Here are a few examples of possible things to try (again, non-exhaustive): Visualize the policy's training performance (e.g., with tensorboard ) Visualize the value network's training performance (if you are using an actor-critic algorithm like PPO) Alter the hand-crafted reward function by stating a hypothesis, changing the reward calculation, and retraining. See Antonin Raffin's blogpost for his explanation of the current reward function An example hypothesis: perhaps the current implementation of the 'smooth' steering constraint is leading to frequent performance collapse - an alternative implementation may do better. Quantify the variance of a trained policy E.g., what is the distribution of collected reward across multiple trajectories using a trained policy? Can this be used to inform further training of the policy? Change the network to use pixels directly instead of using the VAE encoding. Suggestion: Consider using a CNN instead of a dense network, and explore augmentation/subsampling. Train a policy that can drive on random roads (the simulator is currently set up to use the same road for every episode) Replace the pre-trained VAE with one you trained yourself on collected data in the simulator (this is the first component of Part 5 below) Part 5: Retraining the VAE You can sample from the pre-trained VAE's manifold with the following command: $ python -m vae.enjoy_latent -vae vae-level-0-dim-32.pkl You can move some of the sliders around and \"generate\" new views of the track by running the encoded representation through the deconvolutional portion of the VAE network. See the video below for an example output: To get some hands-on experience with VAEs, collect a new set of images and train a new VAE, instead of using the pre-trained VAE. Optionally consider using a different network architecture (for instance, some recent VAE research has focused on improved disentangling of latent factors, such as this IBM paper among others). You can use the VAE training script and train on new data via: $ python -m vae.train --n-epochs 50 --verbose 0 --z-size 64 -f path-to-record/folder/ where z-size specifies the number of latent factors in the bottleneck. Next, try to train a new VAE network on real-world data, e.g., using the camera images you collected in the Imitation Learning Lab from a classroom or the full Stata basement track. Visualize samples from the manifold of this VAE. Optional Exercise If you are feeling particularly eager and have the time, you could use this VAE to train the RACECAR in a classroom or even in Stata basement! You might want to set up a few extra Traxxas batteries to keep charging and swapping out, because this could take an hour or more of driving on-policy, physically resetting when the car gets near an obstacle or veers off course, retraining, and repeating.","title":"Reinforcement Learning Lab"},{"location":"reinforcement_learning/#reinforcement-learning-lab","text":"","title":"Reinforcement Learning Lab"},{"location":"reinforcement_learning/#introduction","text":"Objective: This lab exercise introduces deep reinforcement learning (Deep RL) for autonomous driving in simulation, using only a camera for sensing. Reinforcement learning is distinct from imitation learning: here, the robot learns to explore the environment on its own, with practically no prior information about the world or itself. Through exploration and reinforcement of behaviors which net reward, rather than human-provided examples of behavior to imitate, a robot has the potential to learn novel, optimal techniques which exceed the abilities of humans. Atari games, Go, and StarCraft are a few well-known settings in which Deep RL algorithms have approached or surpassed human expertise. This lab relies on providing the robot with a simulation environment to use as a sandbox for exploration. In particular, we will use a Unity-based simulation environment originally developed by Tawn Kramer for the DonkeyCar RC platform . This lab exercise relies on a Deep RL demonstration by Antonin Raffin , which uses Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC) to quickly train the simulated DonkeyCar to drive on a randomly generated track (and also leverages a variational autoencoder based on code from hardmaru ). This demonstration is itself a fork of an earlier repository by Roma Sokolkov , which leveraged another Deep RL algorithm Deep Deterministic Policy Gradient (DDPG) in the simulator. It is instructive to review Antonin Raffin's blogpost regarding the testing he conducted, for ideas and background, as you work on the lab. For the lab exercise we have forked Antonin Raffin's repository in case there are any lab-specific changes to distribute, but the lab fork is otherwise simply tracking the upstream repo by the original authors (Raffin, Sokolov, Kramer, and other contributors/sources ):","title":"Introduction"},{"location":"reinforcement_learning/#cloning-the-lab-locally","text":"$ git clone https://github.com/mmaz/learning-to-drive-in-5-minutes","title":"Cloning the lab locally:"},{"location":"reinforcement_learning/#a-review-of-reinforcement-learning","text":"If needed, OpenAI's Spinning Up in Deep RL is an excellent way to review in greater depth the concepts discussed during lecture. In particular, the lab is based on topics covered in these sections: Basic concepts in RL The derivation of policy gradients The \"vanilla PG\" algorithm Proximal Policy Optimization (PPO) Soft Actor-Critic (SAC)","title":"A review of Reinforcement Learning"},{"location":"reinforcement_learning/#gym-interface","text":"\"Gym\" interfaces refer to a de facto-standard for reinforcement learning in simulation, popularized by OpenAI's Gym environment . Simulation environments frequently support variants of the following two calls: reset() step() Calls to step() usually take in actions and return the next state, observed reward (if any), and auxillary information such as whether the episode is over (allowing the RL algorithm time to make decisions and optionally reset() the environment for the next episode). For a more in-depth explanation on the concepts in a Gym API, read http://gym.openai.com/docs/ .","title":"Gym Interface"},{"location":"reinforcement_learning/#state-space-dimensionality-reduction","text":"A variational autoencoder or VAE is used to reduce the size of the state space that the policy must consider at training and inference time. The state space is represented as a small vector of floating point numbers (e.g., 20-30) taken from the VAE's bottleneck encoding (i.e., the \"latent space\" encoding), instead of the full image. This lecture from Prof. Ali Ghodsi at the University of Waterloo is an excellent, brief, and self-contained introduction to the theory and implementation of VAEs. For additional reading, please consult one or more of these references (or inquire during office hours/over Slack): Ali Ghodsi's lecture: https://www.youtube.com/watch?v=uaaqyVS9-rM https://jaan.io/what-is-variational-autoencoder-vae-tutorial/ https://www.tensorflow.org/alpha/tutorials/generative/cvae https://blog.keras.io/building-autoencoders-in-keras.html Fast-forward Labs blog Part 1 and Part 2 hardmaru 's series of posts on world models: http://blog.otoro.net//2018/06/09/world-models-experiments/","title":"State-space Dimensionality Reduction"},{"location":"reinforcement_learning/#part-1-downloading-the-donkeycar-simulation-environment","text":"The following link is a Linux build of the Unity Donkey simulation environment from the original author: Download on Google Drive Note The simulation can also be built from source for other platforms, from the donkey tree of the sdsandbox repo using the Unity development platform.","title":"Part 1: Downloading the DonkeyCar simulation environment"},{"location":"reinforcement_learning/#starting-the-simulator","text":"After unzipping the folder and changing into the directory, launch the simulator with: $ ./build_sdsandbox.x86_64 I suggest the following settings of 640x480 , windowed , and Fantastic rendering quality (but this wll depend on your graphics support):","title":"Starting the simulator"},{"location":"reinforcement_learning/#simulator-implementations-of-openai-gym-functions","text":"The following links to the simulator's Gym API implementation are provided as reference for your experimentation (changing the implementation is not necessary however). Note that editing these implementations will not require rebuilidng the simulator, making experimentation easier to conduct. step() is implemented through several callbacks: take_action() calc_reward() - note that this depends implicitly on the cross-track error reported by the simulator on_telemetry() This recieves data from the simulator, including front-bumper images from the simulated DonkeyCar current steering angle and velocity cross-track error (\"cte\") reset() sets all counters to zero is_game_over() is simply a combination of checking for collisions (not present in level 0) or crossing a threshhold of tolerated cross-track error","title":"Simulator implementations of OpenAI Gym functions:"},{"location":"reinforcement_learning/#part-2-installing-deep-rl-python-dependencies","text":"Heads up! If you are using an account on the NEET server, skip this step! These dependencies are already installed. If you do not have a GPU on your computer: # Use TensorFlow without a GPU $ conda env create -f environment.yml Otherwise, if you do have a GPU: # Use TensorFlow with a GPU $ conda env create -f environment-gpu.yml","title":"Part 2: Installing Deep RL python dependencies"},{"location":"reinforcement_learning/#part-3-training-a-policy-with-a-pre-trained-vae","text":"First, download the pre-trained VAE from the author's Google Drive folder for Level 0 in the same directory you cloned the repository into. Next, launch the Unity environment (if it is not already running from Part 1 ) To launch a newly initialized training run for the PPO algorithm across 5000 iterations in Level 0 of the simulator, run the following command: $ python train.py --algo ppo -vae vae-level-0-dim-32.pkl -n 5000 Alternatively, to launch a training run for Soft Actor-Critic (SAC): $ python train.py --algo sac -vae vae-level-0-dim-32.pkl -n 5000 For Level 0 , the simulator will select a random track layout, and the Gym API will be used to reset the simulator using this track layout for each episode when training a new policy. Example Track Layout 1 Example Track Layout 2 Question In addition to accumulated reward, what metrics could be used for measuring a policy's performance? Are 5000 steps enough to train a good policy for PPO or SAC? Across different training runs (each starting from a randomly initialized policy), how frequently will PPO or SAC converge on a good policy, versus suffering from performance collapse?","title":"Part 3: Training a policy with a pre-trained VAE"},{"location":"reinforcement_learning/#part-4-experimenting-with-deep-rl","text":"Once you have tried training a policy in the simulation environment, you can experiment with changing the existing algorithms, or try a different Deep RL algorithm altogether, such as TRPO , TD3 , etc. The goal of this section of the lab is to gain some intuition and experience with training the vehicle's policy using deep reinforcement learning, through modifying the existing code, hyperparameters, and algorithms, or by incorporating new algorithms. Your experimentation can target one or more threads of investigation (this is a non-exhaustive list): How can the training performance (accuracy/speed/etc) and learned policy's effectiveness be visualized or quantified? What is the effect of tuning hyperparameters on the convergence time and robustness (or lack thereof) of algorithms like PPO and SAC? What changes to the algorithm can be made to improve convergence behaviors and the robustness of the learned policy? Here are a few examples of possible things to try (again, non-exhaustive): Visualize the policy's training performance (e.g., with tensorboard ) Visualize the value network's training performance (if you are using an actor-critic algorithm like PPO) Alter the hand-crafted reward function by stating a hypothesis, changing the reward calculation, and retraining. See Antonin Raffin's blogpost for his explanation of the current reward function An example hypothesis: perhaps the current implementation of the 'smooth' steering constraint is leading to frequent performance collapse - an alternative implementation may do better. Quantify the variance of a trained policy E.g., what is the distribution of collected reward across multiple trajectories using a trained policy? Can this be used to inform further training of the policy? Change the network to use pixels directly instead of using the VAE encoding. Suggestion: Consider using a CNN instead of a dense network, and explore augmentation/subsampling. Train a policy that can drive on random roads (the simulator is currently set up to use the same road for every episode) Replace the pre-trained VAE with one you trained yourself on collected data in the simulator (this is the first component of Part 5 below)","title":"Part 4: Experimenting with Deep RL"},{"location":"reinforcement_learning/#part-5-retraining-the-vae","text":"You can sample from the pre-trained VAE's manifold with the following command: $ python -m vae.enjoy_latent -vae vae-level-0-dim-32.pkl You can move some of the sliders around and \"generate\" new views of the track by running the encoded representation through the deconvolutional portion of the VAE network. See the video below for an example output: To get some hands-on experience with VAEs, collect a new set of images and train a new VAE, instead of using the pre-trained VAE. Optionally consider using a different network architecture (for instance, some recent VAE research has focused on improved disentangling of latent factors, such as this IBM paper among others). You can use the VAE training script and train on new data via: $ python -m vae.train --n-epochs 50 --verbose 0 --z-size 64 -f path-to-record/folder/ where z-size specifies the number of latent factors in the bottleneck. Next, try to train a new VAE network on real-world data, e.g., using the camera images you collected in the Imitation Learning Lab from a classroom or the full Stata basement track. Visualize samples from the manifold of this VAE. Optional Exercise If you are feeling particularly eager and have the time, you could use this VAE to train the RACECAR in a classroom or even in Stata basement! You might want to set up a few extra Traxxas batteries to keep charging and swapping out, because this could take an hour or more of driving on-policy, physically resetting when the car gets near an obstacle or veers off course, retraining, and repeating.","title":"Part 5: Retraining the VAE"},{"location":"resources/","text":"Resources You can contribute to this list! Do you have tips, resources, or corrections to add? Did you find something frustrating or unhelpful and want to remove it? Notice any typos? Pull-requests welcome! Submit one at https://github.com/mmaz/neet UAV Course website: http://neet.beaver.works If this is your first time using a shell, git, or ssh, here are a few tutorials to help acquaint yourself with the basics. Tip: 2x speed videos On youtube, click the gear icon and change the playback speed to 2x, which is useful when watching slower-paced videos. Python MIT's own intro to python and programming course is a good place to review python basics, such as branching, iteration, and lists. Videos are also here: youtube playlist The Shell The shell (aka, command line or terminal) is how we connect to the UAV. Shell basics: https://www.youtube.com/watch?v=poT5Yd0Ag8I https://www.youtube.com/watch?v=oxuRxtrO2Ag https://www.digitalocean.com/community/tutorials/how-to-use-cd-pwd-and-ls-to-explore-the-file-system-on-a-linux-server Some useful commands to learn are: ls, cd, pwd, find, grep, mkdir, rm, mv, cp, less, cat, which Shell operators like | (the pipe symbol) and > as well as control signals, like Ctrl+C , are also useful to know. You may need to occasionally find your IP address and check for network reachability. Some useful commands for this: ping, ifconfig, wget, curl For example, $ ping drone.beaver.works I often find my IP address with the following command: ifconfig | grep inet How does this work? ifconfig spits out a lot of network configuration information. The | symbol pipes the output of ifconfig to grep , which searches for the word 'inet' on each line in the output. This corresponds to a list of IP addresses associated with my computer. On linux you can also use hostname -I \u2013 in general there can be many ways to find the same information using the command line, with different tradeoffs. This cheatsheet (or others, just google for 'bash cheat sheet') may come in handy: https://gist.github.com/LeCoupa/122b12050f5fb267e75f SSH and SCP: https://www.youtube.com/watch?v=rm6pewTcSro Git https://www.youtube.com/watch?v=zbKdDsNNOhg (answers to \u201cwhy use Git in the first place?\u201d) https://www.youtube.com/watch?v=3a2x1iJFJWc https://youtu.be/9pa_PV2LUlw Writing Python code You\u2019ll want to pick a code editor. Visual Studio Code, Sublime, Atom, GEdit, vim, and emacs are all popular choices. https://code.visualstudio.com/ is easy to install and configure, available on all platforms, and free. ROS (Robot Operating System) One-hour introduction to ROS: https://www.youtube.com/watch?v=0BxVPCInS3M","title":"Resources"},{"location":"resources/#resources","text":"You can contribute to this list! Do you have tips, resources, or corrections to add? Did you find something frustrating or unhelpful and want to remove it? Notice any typos? Pull-requests welcome! Submit one at https://github.com/mmaz/neet UAV Course website: http://neet.beaver.works If this is your first time using a shell, git, or ssh, here are a few tutorials to help acquaint yourself with the basics. Tip: 2x speed videos On youtube, click the gear icon and change the playback speed to 2x, which is useful when watching slower-paced videos.","title":"Resources"},{"location":"resources/#python","text":"MIT's own intro to python and programming course is a good place to review python basics, such as branching, iteration, and lists. Videos are also here: youtube playlist","title":"Python"},{"location":"resources/#the-shell","text":"The shell (aka, command line or terminal) is how we connect to the UAV. Shell basics: https://www.youtube.com/watch?v=poT5Yd0Ag8I https://www.youtube.com/watch?v=oxuRxtrO2Ag https://www.digitalocean.com/community/tutorials/how-to-use-cd-pwd-and-ls-to-explore-the-file-system-on-a-linux-server Some useful commands to learn are: ls, cd, pwd, find, grep, mkdir, rm, mv, cp, less, cat, which Shell operators like | (the pipe symbol) and > as well as control signals, like Ctrl+C , are also useful to know. You may need to occasionally find your IP address and check for network reachability. Some useful commands for this: ping, ifconfig, wget, curl For example, $ ping drone.beaver.works I often find my IP address with the following command: ifconfig | grep inet How does this work? ifconfig spits out a lot of network configuration information. The | symbol pipes the output of ifconfig to grep , which searches for the word 'inet' on each line in the output. This corresponds to a list of IP addresses associated with my computer. On linux you can also use hostname -I \u2013 in general there can be many ways to find the same information using the command line, with different tradeoffs. This cheatsheet (or others, just google for 'bash cheat sheet') may come in handy: https://gist.github.com/LeCoupa/122b12050f5fb267e75f","title":"The Shell"},{"location":"resources/#ssh-and-scp","text":"https://www.youtube.com/watch?v=rm6pewTcSro","title":"SSH and SCP:"},{"location":"resources/#git","text":"https://www.youtube.com/watch?v=zbKdDsNNOhg (answers to \u201cwhy use Git in the first place?\u201d) https://www.youtube.com/watch?v=3a2x1iJFJWc https://youtu.be/9pa_PV2LUlw","title":"Git"},{"location":"resources/#writing-python-code","text":"You\u2019ll want to pick a code editor. Visual Studio Code, Sublime, Atom, GEdit, vim, and emacs are all popular choices. https://code.visualstudio.com/ is easy to install and configure, available on all platforms, and free.","title":"Writing Python code"},{"location":"resources/#ros-robot-operating-system","text":"One-hour introduction to ROS: https://www.youtube.com/watch?v=0BxVPCInS3M","title":"ROS (Robot Operating System)"},{"location":"server/","text":"NEET Spring 2019 Server Keep this in mind! The NEET Server is a shared resource! Coordinate the times you are using the server with the other NEET teams so that you do not overlap. And remember to shut down your Jupyter Notebook or training script after you are done. Do not leave a training script running - it will reserve all available GPU memory, and prevent other teams from training. If you and another team agree beforehand, you can limit the memory TensorFlow allocates while training, and both teams can train on the server simultaneously. Still, in this case, remember to shut down jupyter or your python training script when you are done. Ask the instructors before running any commands with sudo rights. Lastly, please do not reboot the server! Again, ask if you have questions. Installing Dependencies There's nothing to do! The dependencies you need for the Imitation learning lab and the Reinforcement learning lab are already installed in the base conda environment. This environment is automatically activated when you log in, so there are no extra steps necessary (i.e., you do not need to run conda activate base ). ( conda list will show all dependencies installed in the environment if you are curious.) Connecting to the server over SSH At the terminal, the following command will SSH into the server, enable x-forwarding, and also forward a port for Jupyter Notebook access: Note You will need to replace $USERNAME and $IP_ADDRESS below with the appropriate values (ask the instructors in the Slack chatroom) ssh -L 8888:localhost:8888 -X $USERNAME@$IP_ADDRESS Once you are connected, run ls - you have an empty directory for your team already created in the home directory. ( mx0 is the \"instructor\" directory) $ ls donkey_simulator mx0 mx1 mx2 mx3 Adding a shortcut for easy SSH access: This step is optional You can simplify the process of connecting to the NEET server further by adding the following information to your local SSH config file: Warning Make sure you are editing the config file on your personal laptop (not the server!) before proceeding: $ vi ~/.ssh/config Note You will need to replace $USERNAME and $IP_ADDRESS below with the appropriate values (ask the instructors in the Slack chatroom) Host neet User $USERNAME HostName $IP_ADDRESS ForwardX11 yes IdentityFile ~/.ssh/id_rsa LocalForward 8888 127.0.0.1:8888 ControlPath ~/.ssh/controlmasters_%r@%h:%p ControlMaster auto This sets up port-forwarding (for Jupyter), SSH multiplexing , and X-forwarding once and for all. Afterwards you can SSH into the server just by typing: $ ssh neet at your local command prompt. Convenient SCP With the shortcut, you can also SCP directories and files to the server easily, e.g., to copy to your team's directory mxN (where N is 1,2, or 3): $ scp -r local_directory_w_training_images neet:~/mxN/ No-password logins If you would like to avoid being asked for a password each time, you can generate a local identity file: https://help.github.com/en/articles/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent Then, use the following command to connect to the server. # only required once $ ssh-copy-id neet which will copy your public key to the server. Opening Jupyter Notebooks over SSH Note If you have Jupyter Notebook running locally, shut it down first (or restart it and change the port to something besides the default port 8888 , so that the local port used by Jupyter does not conflict with the SSH-forwarded one) On your local computer: jupyter notebook --port 8889 for example. Once you have SSHed into the computer, you can start jupyter in your team's folder (include --no-browser so that Firefox does not try to load over SSH - which will be annoyingly slow) $ cd mxN/ $ jupyter notebook --no-browser Jupyter will provide a URL for you to use in your local computer's browser. Copy and paste it into your browser, e.g., Or copy and paste one of these URLs: http://localhost:8888/?token=r4nd0mh3xstring Note It is highly recommended to run a program like tmux or screen after first logging in, so that your work is not lost in case your SSH connection is interrupted. tmux is already installed on the server. Here is a simple guide that introduces tmux . Also, here is a cheatsheet for quick reference. $ cd mxN/ $ tmux $ jupyter notebook --no-browser Running the RL Simulation Environment You can collect data and train a policy for the RL lab in person on the server (contact the instructors over Slack regarding server access). There is a folder already created in the home directory called ~/donkey_simulator/ - you can start the simulator by double-clicking the executable build_sdsandbox.x86_64 in the Ubuntu file explorer or by running it in a new terminal. Once the simulator is running, you can train a policy in another terminal or over SSH. Remember Shut down the simulator and your training scripts when you are done!","title":"NEET Spring 2019 Server"},{"location":"server/#neet-spring-2019-server","text":"Keep this in mind! The NEET Server is a shared resource! Coordinate the times you are using the server with the other NEET teams so that you do not overlap. And remember to shut down your Jupyter Notebook or training script after you are done. Do not leave a training script running - it will reserve all available GPU memory, and prevent other teams from training. If you and another team agree beforehand, you can limit the memory TensorFlow allocates while training, and both teams can train on the server simultaneously. Still, in this case, remember to shut down jupyter or your python training script when you are done. Ask the instructors before running any commands with sudo rights. Lastly, please do not reboot the server! Again, ask if you have questions.","title":"NEET Spring 2019 Server"},{"location":"server/#installing-dependencies","text":"There's nothing to do! The dependencies you need for the Imitation learning lab and the Reinforcement learning lab are already installed in the base conda environment. This environment is automatically activated when you log in, so there are no extra steps necessary (i.e., you do not need to run conda activate base ). ( conda list will show all dependencies installed in the environment if you are curious.)","title":"Installing Dependencies"},{"location":"server/#connecting-to-the-server-over-ssh","text":"At the terminal, the following command will SSH into the server, enable x-forwarding, and also forward a port for Jupyter Notebook access: Note You will need to replace $USERNAME and $IP_ADDRESS below with the appropriate values (ask the instructors in the Slack chatroom) ssh -L 8888:localhost:8888 -X $USERNAME@$IP_ADDRESS Once you are connected, run ls - you have an empty directory for your team already created in the home directory. ( mx0 is the \"instructor\" directory) $ ls donkey_simulator mx0 mx1 mx2 mx3","title":"Connecting to the server over SSH"},{"location":"server/#adding-a-shortcut-for-easy-ssh-access","text":"This step is optional You can simplify the process of connecting to the NEET server further by adding the following information to your local SSH config file: Warning Make sure you are editing the config file on your personal laptop (not the server!) before proceeding: $ vi ~/.ssh/config Note You will need to replace $USERNAME and $IP_ADDRESS below with the appropriate values (ask the instructors in the Slack chatroom) Host neet User $USERNAME HostName $IP_ADDRESS ForwardX11 yes IdentityFile ~/.ssh/id_rsa LocalForward 8888 127.0.0.1:8888 ControlPath ~/.ssh/controlmasters_%r@%h:%p ControlMaster auto This sets up port-forwarding (for Jupyter), SSH multiplexing , and X-forwarding once and for all. Afterwards you can SSH into the server just by typing: $ ssh neet at your local command prompt.","title":"Adding a shortcut for easy SSH access:"},{"location":"server/#convenient-scp","text":"With the shortcut, you can also SCP directories and files to the server easily, e.g., to copy to your team's directory mxN (where N is 1,2, or 3): $ scp -r local_directory_w_training_images neet:~/mxN/","title":"Convenient SCP"},{"location":"server/#no-password-logins","text":"If you would like to avoid being asked for a password each time, you can generate a local identity file: https://help.github.com/en/articles/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent Then, use the following command to connect to the server. # only required once $ ssh-copy-id neet which will copy your public key to the server.","title":"No-password logins"},{"location":"server/#opening-jupyter-notebooks-over-ssh","text":"Note If you have Jupyter Notebook running locally, shut it down first (or restart it and change the port to something besides the default port 8888 , so that the local port used by Jupyter does not conflict with the SSH-forwarded one) On your local computer: jupyter notebook --port 8889 for example. Once you have SSHed into the computer, you can start jupyter in your team's folder (include --no-browser so that Firefox does not try to load over SSH - which will be annoyingly slow) $ cd mxN/ $ jupyter notebook --no-browser Jupyter will provide a URL for you to use in your local computer's browser. Copy and paste it into your browser, e.g., Or copy and paste one of these URLs: http://localhost:8888/?token=r4nd0mh3xstring Note It is highly recommended to run a program like tmux or screen after first logging in, so that your work is not lost in case your SSH connection is interrupted. tmux is already installed on the server. Here is a simple guide that introduces tmux . Also, here is a cheatsheet for quick reference. $ cd mxN/ $ tmux $ jupyter notebook --no-browser","title":"Opening Jupyter Notebooks over SSH"},{"location":"server/#running-the-rl-simulation-environment","text":"You can collect data and train a policy for the RL lab in person on the server (contact the instructors over Slack regarding server access). There is a folder already created in the home directory called ~/donkey_simulator/ - you can start the simulator by double-clicking the executable build_sdsandbox.x86_64 in the Ubuntu file explorer or by running it in a new terminal. Once the simulator is running, you can train a policy in another terminal or over SSH. Remember Shut down the simulator and your training scripts when you are done!","title":"Running the RL Simulation Environment"}]}