{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MIT NEET Spring 2019 PilotNet Lab Objective: Vision-only navigation of a racing circuit, based on: https://devblogs.nvidia.com/deep-learning-self-driving-cars/ Nvidia's blog post introducing the concept https://arxiv.org/pdf/1704.07911.pdf Nvidia's PilotNet paper https://github.com/naokishibuya/car-behavioral-cloning (using Udacity's simulator) Introduction This lab provides an introduction to end-to-end imitation learning for vision-only navigation of a racetrack. There are two components to this lab. First, we will learn how to train a model (specifically, a convolutional neural network ) in simulation. This will involve driving a simulated car around a virtual racetrack and collecting camera data from the rendered game engine. Then we will train another model using camera data and steering angles collected from the RACECAR platform in a real-world environment, Stata basement. In simulation: In Stata basement: Training: We will use three cameras mounted on the virtual and real-world RACECAR to collect training data. Excerpting from Nvidia's blog post : Training data contains single images sampled from the video, paired with the corresponding steering command (1/r). Training with data from only the human driver is not sufficient; the network must also learn how to recover from any mistakes, or the car will slowly drift off the road. The training data is therefore augmented with additional images that show the car in different shifts from the center of the lane and rotations from the direction of the road. The images for two specific off-center shifts can be obtained from the left and the right cameras. Additional shifts between the cameras and all rotations are simulated through viewpoint transformation of the image from the nearest camera. Precise viewpoint transformation requires 3D scene knowledge which we don\u2019t have, so we approximate the transformation by assuming all points below the horizon are on flat ground, and all points above the horizon are infinitely far away. This works fine for flat terrain Let us take a closer look at a Keras implementation of the CNN architecture: Using TensorFlow's Keras API , let us look at an implementation of the above network in code: Exercise How many parameters does each layer represent? What is the effect of changing the input size on the total number of parameters in the network? Why? Hint: use model.summary() as a way to explore the effect of changing input size. For more on TensorFlow's Keras API, click here . from tensorflow.keras.layers import Lambda, Conv2D, MaxPooling2D, Dropout, Dense, Flatten from tensorflow.keras.models import Sequential # you will need to crop or shrink images to the dimensions you choose here: IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS = 66, 200, 3 INPUT_SHAPE = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS) def build_model(dropout_rate=0.5): model = Sequential() model.add(Lambda(lambda x: x/127.5-1.0, input_shape=INPUT_SHAPE)) #normalizes image data model.add(Conv2D(24, (5,5), strides=(2, 2), activation='elu')) model.add(Conv2D(36, (5,5), strides=(2, 2), activation='elu')) model.add(Conv2D(48, (5,5), strides=(2, 2), activation='elu')) model.add(Conv2D(64, (3,3), activation='elu')) model.add(Conv2D(64, (3,3), activation='elu')) model.add(Dropout(dropout_rate)) model.add(Flatten()) model.add(Dense(100, activation='elu')) model.add(Dense(50, activation='elu')) model.add(Dense(10, activation='elu')) model.add(Dense(1)) model.summary() return model Note Note that Keras will disable Dropout regularization at inference time. See here for details. Servo histogram It is important to ensure the train/test split of the data you have collected have similar driving condition represented. For instance, here is the histogram of servo angles in the training and testing data used above: Extending to more general environments It is possible to train a network with driving data from public roads, in order to experiment with how it affects the performance of your car in Stata basement. Danger Obviously, you should not test anything on public roads yourself, either on a RACECAR or any other car. Be safe and responsible! Running inference The following script will load a pre-trained model and drive the car through Stata basement: $ # Optional -- mount SSD for data collection $ # sudo mount -t auto /dev/sda1 /media/ssd/ $ roslaunch racecar_bringup base.launch teleop:=true $ roslaunch zed_wrapper zed.launch $ cd ~/pilotnet # or wherever you have basement-006.h5 weights stored $ python pilotnet_drive.py Data collection You will need to save images to the car's SSD: In zed.launch ( $ roscd zed_wrapper ): <arg name=\"resolution\" default=\"3\" /> <!--0=RESOLUTION_HD2K, 1=RESOLUTION_HD1080, 2=RESOLUTION_HD720, 3=RESOLUTION_VGA --> <arg name=\"frame_rate\" default=\"15\" /> In TODO launch/record_bag.launch : args=\"--output-prefix $(arg saveas) $(arg extra_args) /joy /racecar_drive /vesc/sensors/core /velodyne_packets /scan /imu/data_raw /imu/data /imu/mag /zed/left/image_raw_color/compressed /zed/right/image_raw_color/compressed /zed/left/camera_info /zed/right/camera_info\" /> Then, to record the rosbag TODO : $ roslaunch racecar_bringup record_bag.launch saveas:=/media/ssd/rosbags/ Image Augmentation Example transformations: Center Image Left and right Images def choose_image(data_dir, center, left, right, steering_angle): \"\"\" Randomly choose an image from the center, left or right, and adjust the steering angle. \"\"\" choice = np.random.choice(3) if choice == 0: return load_image(data_dir, left), steering_angle + 0.2 elif choice == 1: return load_image(data_dir, right), steering_angle - 0.2 return load_image(data_dir, center), steering_angle Flipped Image if np.random.rand() < 0.5: image = cv2.flip(image, 1) steering_angle = -steering_angle return image, steering_angle Translated Image def random_translate(image, steering_angle, range_x, range_y): \"\"\" Randomly shift the image virtially and horizontally (translation). \"\"\" trans_x = range_x * (np.random.rand() - 0.5) trans_y = range_y * (np.random.rand() - 0.5) steering_angle += trans_x * 0.002 trans_m = np.float32([[1, 0, trans_x], [0, 1, trans_y]]) height, width = image.shape[:2] image = cv2.warpAffine(image, trans_m, (width, height)) return image, steering_angle","title":"MIT NEET Spring 2019"},{"location":"#mit-neet-spring-2019","text":"","title":"MIT NEET Spring 2019"},{"location":"#pilotnet-lab","text":"Objective: Vision-only navigation of a racing circuit, based on: https://devblogs.nvidia.com/deep-learning-self-driving-cars/ Nvidia's blog post introducing the concept https://arxiv.org/pdf/1704.07911.pdf Nvidia's PilotNet paper https://github.com/naokishibuya/car-behavioral-cloning (using Udacity's simulator)","title":"PilotNet Lab"},{"location":"#introduction","text":"This lab provides an introduction to end-to-end imitation learning for vision-only navigation of a racetrack. There are two components to this lab. First, we will learn how to train a model (specifically, a convolutional neural network ) in simulation. This will involve driving a simulated car around a virtual racetrack and collecting camera data from the rendered game engine. Then we will train another model using camera data and steering angles collected from the RACECAR platform in a real-world environment, Stata basement.","title":"Introduction"},{"location":"#in-simulation","text":"","title":"In simulation:"},{"location":"#in-stata-basement","text":"","title":"In Stata basement:"},{"location":"#training","text":"We will use three cameras mounted on the virtual and real-world RACECAR to collect training data. Excerpting from Nvidia's blog post : Training data contains single images sampled from the video, paired with the corresponding steering command (1/r). Training with data from only the human driver is not sufficient; the network must also learn how to recover from any mistakes, or the car will slowly drift off the road. The training data is therefore augmented with additional images that show the car in different shifts from the center of the lane and rotations from the direction of the road. The images for two specific off-center shifts can be obtained from the left and the right cameras. Additional shifts between the cameras and all rotations are simulated through viewpoint transformation of the image from the nearest camera. Precise viewpoint transformation requires 3D scene knowledge which we don\u2019t have, so we approximate the transformation by assuming all points below the horizon are on flat ground, and all points above the horizon are infinitely far away. This works fine for flat terrain Let us take a closer look at a Keras implementation of the CNN architecture: Using TensorFlow's Keras API , let us look at an implementation of the above network in code: Exercise How many parameters does each layer represent? What is the effect of changing the input size on the total number of parameters in the network? Why? Hint: use model.summary() as a way to explore the effect of changing input size. For more on TensorFlow's Keras API, click here . from tensorflow.keras.layers import Lambda, Conv2D, MaxPooling2D, Dropout, Dense, Flatten from tensorflow.keras.models import Sequential # you will need to crop or shrink images to the dimensions you choose here: IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS = 66, 200, 3 INPUT_SHAPE = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS) def build_model(dropout_rate=0.5): model = Sequential() model.add(Lambda(lambda x: x/127.5-1.0, input_shape=INPUT_SHAPE)) #normalizes image data model.add(Conv2D(24, (5,5), strides=(2, 2), activation='elu')) model.add(Conv2D(36, (5,5), strides=(2, 2), activation='elu')) model.add(Conv2D(48, (5,5), strides=(2, 2), activation='elu')) model.add(Conv2D(64, (3,3), activation='elu')) model.add(Conv2D(64, (3,3), activation='elu')) model.add(Dropout(dropout_rate)) model.add(Flatten()) model.add(Dense(100, activation='elu')) model.add(Dense(50, activation='elu')) model.add(Dense(10, activation='elu')) model.add(Dense(1)) model.summary() return model Note Note that Keras will disable Dropout regularization at inference time. See here for details.","title":"Training:"},{"location":"#servo-histogram","text":"It is important to ensure the train/test split of the data you have collected have similar driving condition represented. For instance, here is the histogram of servo angles in the training and testing data used above:","title":"Servo histogram"},{"location":"#extending-to-more-general-environments","text":"It is possible to train a network with driving data from public roads, in order to experiment with how it affects the performance of your car in Stata basement. Danger Obviously, you should not test anything on public roads yourself, either on a RACECAR or any other car. Be safe and responsible!","title":"Extending to more general environments"},{"location":"#running-inference","text":"The following script will load a pre-trained model and drive the car through Stata basement: $ # Optional -- mount SSD for data collection $ # sudo mount -t auto /dev/sda1 /media/ssd/ $ roslaunch racecar_bringup base.launch teleop:=true $ roslaunch zed_wrapper zed.launch $ cd ~/pilotnet # or wherever you have basement-006.h5 weights stored $ python pilotnet_drive.py","title":"Running inference"},{"location":"#data-collection","text":"You will need to save images to the car's SSD: In zed.launch ( $ roscd zed_wrapper ): <arg name=\"resolution\" default=\"3\" /> <!--0=RESOLUTION_HD2K, 1=RESOLUTION_HD1080, 2=RESOLUTION_HD720, 3=RESOLUTION_VGA --> <arg name=\"frame_rate\" default=\"15\" /> In TODO launch/record_bag.launch : args=\"--output-prefix $(arg saveas) $(arg extra_args) /joy /racecar_drive /vesc/sensors/core /velodyne_packets /scan /imu/data_raw /imu/data /imu/mag /zed/left/image_raw_color/compressed /zed/right/image_raw_color/compressed /zed/left/camera_info /zed/right/camera_info\" /> Then, to record the rosbag TODO : $ roslaunch racecar_bringup record_bag.launch saveas:=/media/ssd/rosbags/","title":"Data collection"},{"location":"#image-augmentation","text":"Example transformations: Center Image Left and right Images def choose_image(data_dir, center, left, right, steering_angle): \"\"\" Randomly choose an image from the center, left or right, and adjust the steering angle. \"\"\" choice = np.random.choice(3) if choice == 0: return load_image(data_dir, left), steering_angle + 0.2 elif choice == 1: return load_image(data_dir, right), steering_angle - 0.2 return load_image(data_dir, center), steering_angle Flipped Image if np.random.rand() < 0.5: image = cv2.flip(image, 1) steering_angle = -steering_angle return image, steering_angle Translated Image def random_translate(image, steering_angle, range_x, range_y): \"\"\" Randomly shift the image virtially and horizontally (translation). \"\"\" trans_x = range_x * (np.random.rand() - 0.5) trans_y = range_y * (np.random.rand() - 0.5) steering_angle += trans_x * 0.002 trans_m = np.float32([[1, 0, trans_x], [0, 1, trans_y]]) height, width = image.shape[:2] image = cv2.warpAffine(image, trans_m, (width, height)) return image, steering_angle","title":"Image Augmentation"},{"location":"neet_fall_2018/","text":"MIT NEET Fall 2018 Multiplex SSH This is highly recommended for connecting to the drones: Create or edit a file in the .ssh/ directory inside your home directory, called config : $ nano ~/.ssh/config or $ gedit ~/.ssh/config or $ vi ~/.ssh/config (or whichever text editor you are comfortable with) You will need to add two lines to the file, and you will need to replace PATH_TO_HOME with the correct path for your platform: on linux, /home/YOURTEAMNAME/.ssh/ (or your username, if you are not using a team laptop) on mac, /Users/YOURUSERNAME/.ssh windows, (TODO) Add the following two lines ( Remember: change PATH_TO_HOME as specified above): ControlMaster auto ControlPath PATH_TO_HOME/.ssh/ssh_mux_%h_%p_%r Important - remember to start your FIRST SSH connection with -Y if you plan to use xforwarding (e.g., for rqt_image_view ) Directory Setup These instructions replace this section on the website. On the SSH window on your team laptop, enter the following commands cd ~ cd ~/bwsi-uav/catkin_ws/src git clone https://github.com/BWSI-UAV/aero_control.git cd aero_control git remote add upstream https://github.com/BWSI-UAV/aero_control.git cd ~/bwsi-uav git clone https://github.com/BWSI-UAV/laboratory.git cd laboratory git remote add upstream https://github.com/BWSI-UAV/laboratory.git cd ~/bwsi-uav git clone https://github.com/BWSI-UAV/documents.git cd documents git remote add upstream https://github.com/BWSI-UAV/documents.git Compressed Camera Feeds To find if your drone supports compressed camera feeds: Start roscore Start optical flow: sudo -E ~/bwsi-uav/catkin-ws/src/aero-optical-flow/build/aero-optical-flow $ rostopic list | grep compressed If you don't see /aero_downward_camera/image/compressed in the results you will need to install compressed transport support: sudo apt-get install ros-kinetic-image-transport-plugins then restart your camera feed by restarting the aero-optical-flow binary (step 1 above). To record a compressed downward camera feed: $ cd ~/rosbags/ # or wherever you want to store your rosbag $ time rosbag record -O downward /aero_downward_camera/image/compressed # -O specifies the filename You can then SCP your rosbag to your team laptop. To convert compressed camera messages to OpenCV images, you can't use CVBridge. Here is an OpenCV-specific decoding solution. (You could also use CompressedImage from sensor_msgs.msg ): from __future__ import print_function import cv2 import numpy as np import roslib import rospy from sensor_msgs.msg import CompressedImage # We do not use cv_bridge since it does not support CompressedImage # from cv_bridge import CvBridge, CvBridgeError import rosbag import os DEST = \"/path/to/folder/to/save/images\" BAG = \"/path/to/rosbag.bag\" #your camera topic: CAM = '/aero_downward_camera/image/compressed' def bag2msgs(): bag = rosbag.Bag(BAG) if bag is None: raise ValueError(\"no bag {}\".format(BAG)) msgs = [] for topic, msg, t in bag.read_messages(topics=[CAM]): msgs.append(msg) bag.close() print(\"MESSAGES: {}\".format(len(msgs))) return msgs def uncompress(msgs): imgs = [] for msg in msgs: #### direct conversion to CV2 #### np_arr = np.fromstring(msg.data, np.uint8) image_np = cv2.imdecode(np_arr, cv2.IMREAD_COLOR) # OpenCV >= 3.0: imgs.append(image_np) return imgs if __name__ == \"__main__\": if os.listdir(DEST) != []: raise ValueError('need empty directory for dest {}'.format(DEST)) msgs = bag2msgs() imgs = uncompress(msgs) for idx,im in enumerate(imgs): if idx % 50 == 0: print(idx) imname = \"frame{:05d}.jpg\".format(idx) cv2.imwrite(DEST + imname, im)","title":"MIT NEET Fall 2018"},{"location":"neet_fall_2018/#mit-neet-fall-2018","text":"","title":"MIT NEET Fall 2018"},{"location":"neet_fall_2018/#multiplex-ssh","text":"This is highly recommended for connecting to the drones: Create or edit a file in the .ssh/ directory inside your home directory, called config : $ nano ~/.ssh/config or $ gedit ~/.ssh/config or $ vi ~/.ssh/config (or whichever text editor you are comfortable with) You will need to add two lines to the file, and you will need to replace PATH_TO_HOME with the correct path for your platform: on linux, /home/YOURTEAMNAME/.ssh/ (or your username, if you are not using a team laptop) on mac, /Users/YOURUSERNAME/.ssh windows, (TODO) Add the following two lines ( Remember: change PATH_TO_HOME as specified above): ControlMaster auto ControlPath PATH_TO_HOME/.ssh/ssh_mux_%h_%p_%r Important - remember to start your FIRST SSH connection with -Y if you plan to use xforwarding (e.g., for rqt_image_view )","title":"Multiplex SSH"},{"location":"neet_fall_2018/#directory-setup","text":"These instructions replace this section on the website. On the SSH window on your team laptop, enter the following commands cd ~ cd ~/bwsi-uav/catkin_ws/src git clone https://github.com/BWSI-UAV/aero_control.git cd aero_control git remote add upstream https://github.com/BWSI-UAV/aero_control.git cd ~/bwsi-uav git clone https://github.com/BWSI-UAV/laboratory.git cd laboratory git remote add upstream https://github.com/BWSI-UAV/laboratory.git cd ~/bwsi-uav git clone https://github.com/BWSI-UAV/documents.git cd documents git remote add upstream https://github.com/BWSI-UAV/documents.git","title":"Directory Setup"},{"location":"neet_fall_2018/#compressed-camera-feeds","text":"To find if your drone supports compressed camera feeds: Start roscore Start optical flow: sudo -E ~/bwsi-uav/catkin-ws/src/aero-optical-flow/build/aero-optical-flow $ rostopic list | grep compressed If you don't see /aero_downward_camera/image/compressed in the results you will need to install compressed transport support: sudo apt-get install ros-kinetic-image-transport-plugins then restart your camera feed by restarting the aero-optical-flow binary (step 1 above). To record a compressed downward camera feed: $ cd ~/rosbags/ # or wherever you want to store your rosbag $ time rosbag record -O downward /aero_downward_camera/image/compressed # -O specifies the filename You can then SCP your rosbag to your team laptop. To convert compressed camera messages to OpenCV images, you can't use CVBridge. Here is an OpenCV-specific decoding solution. (You could also use CompressedImage from sensor_msgs.msg ): from __future__ import print_function import cv2 import numpy as np import roslib import rospy from sensor_msgs.msg import CompressedImage # We do not use cv_bridge since it does not support CompressedImage # from cv_bridge import CvBridge, CvBridgeError import rosbag import os DEST = \"/path/to/folder/to/save/images\" BAG = \"/path/to/rosbag.bag\" #your camera topic: CAM = '/aero_downward_camera/image/compressed' def bag2msgs(): bag = rosbag.Bag(BAG) if bag is None: raise ValueError(\"no bag {}\".format(BAG)) msgs = [] for topic, msg, t in bag.read_messages(topics=[CAM]): msgs.append(msg) bag.close() print(\"MESSAGES: {}\".format(len(msgs))) return msgs def uncompress(msgs): imgs = [] for msg in msgs: #### direct conversion to CV2 #### np_arr = np.fromstring(msg.data, np.uint8) image_np = cv2.imdecode(np_arr, cv2.IMREAD_COLOR) # OpenCV >= 3.0: imgs.append(image_np) return imgs if __name__ == \"__main__\": if os.listdir(DEST) != []: raise ValueError('need empty directory for dest {}'.format(DEST)) msgs = bag2msgs() imgs = uncompress(msgs) for idx,im in enumerate(imgs): if idx % 50 == 0: print(idx) imname = \"frame{:05d}.jpg\".format(idx) cv2.imwrite(DEST + imname, im)","title":"Compressed Camera Feeds"},{"location":"resources/","text":"Resources You can contribute to this list! Do you have tips, resources, or corrections to add? Did you find something frustrating or unhelpful and want to remove it? Notice any typos? Pull-requests welcome! Submit one at https://github.com/mmaz/neet UAV Course website: http://neet.beaver.works If this is your first time using a shell, git, or ssh, here are a few tutorials to help acquaint yourself with the basics. Tip: 2x speed videos On youtube, click the gear icon and change the playback speed to 2x, which is useful when watching slower-paced videos. Python MIT's own intro to python and programming course is a good place to review python basics, such as branching, iteration, and lists. Videos are also here: youtube playlist The Shell The shell (aka, command line or terminal) is how we connect to the UAV. Shell basics: https://www.youtube.com/watch?v=poT5Yd0Ag8I https://www.youtube.com/watch?v=oxuRxtrO2Ag https://www.digitalocean.com/community/tutorials/how-to-use-cd-pwd-and-ls-to-explore-the-file-system-on-a-linux-server Some useful commands to learn are: ls, cd, pwd, find, grep, mkdir, rm, mv, cp, less, cat, which Shell operators like | (the pipe symbol) and > as well as control signals, like Ctrl+C , are also useful to know. You may need to occasionally find your IP address and check for network reachability. Some useful commands for this: ping, ifconfig, wget, curl For example, $ ping drone.beaver.works I often find my IP address with the following command: ifconfig | grep inet How does this work? ifconfig spits out a lot of network configuration information. The | symbol pipes the output of ifconfig to grep , which searches for the word 'inet' on each line in the output. This corresponds to a list of IP addresses associated with my computer. On linux you can also use hostname -I \u2013 in general there can be many ways to find the same information using the command line, with different tradeoffs. This cheatsheet (or others, just google for 'bash cheat sheet') may come in handy: https://gist.github.com/LeCoupa/122b12050f5fb267e75f SSH and SCP: https://www.youtube.com/watch?v=rm6pewTcSro Git https://www.youtube.com/watch?v=zbKdDsNNOhg (answers to \u201cwhy use Git in the first place?\u201d) https://www.youtube.com/watch?v=3a2x1iJFJWc https://youtu.be/9pa_PV2LUlw Writing Python code You\u2019ll want to pick a code editor. Visual Studio Code, Sublime, Atom, GEdit, vim, and emacs are all popular choices. https://code.visualstudio.com/ is easy to install and configure, available on all platforms, and free. ROS (Robot Operating System) One-hour introduction to ROS: https://www.youtube.com/watch?v=0BxVPCInS3M","title":"Resources"},{"location":"resources/#resources","text":"You can contribute to this list! Do you have tips, resources, or corrections to add? Did you find something frustrating or unhelpful and want to remove it? Notice any typos? Pull-requests welcome! Submit one at https://github.com/mmaz/neet UAV Course website: http://neet.beaver.works If this is your first time using a shell, git, or ssh, here are a few tutorials to help acquaint yourself with the basics. Tip: 2x speed videos On youtube, click the gear icon and change the playback speed to 2x, which is useful when watching slower-paced videos.","title":"Resources"},{"location":"resources/#python","text":"MIT's own intro to python and programming course is a good place to review python basics, such as branching, iteration, and lists. Videos are also here: youtube playlist","title":"Python"},{"location":"resources/#the-shell","text":"The shell (aka, command line or terminal) is how we connect to the UAV. Shell basics: https://www.youtube.com/watch?v=poT5Yd0Ag8I https://www.youtube.com/watch?v=oxuRxtrO2Ag https://www.digitalocean.com/community/tutorials/how-to-use-cd-pwd-and-ls-to-explore-the-file-system-on-a-linux-server Some useful commands to learn are: ls, cd, pwd, find, grep, mkdir, rm, mv, cp, less, cat, which Shell operators like | (the pipe symbol) and > as well as control signals, like Ctrl+C , are also useful to know. You may need to occasionally find your IP address and check for network reachability. Some useful commands for this: ping, ifconfig, wget, curl For example, $ ping drone.beaver.works I often find my IP address with the following command: ifconfig | grep inet How does this work? ifconfig spits out a lot of network configuration information. The | symbol pipes the output of ifconfig to grep , which searches for the word 'inet' on each line in the output. This corresponds to a list of IP addresses associated with my computer. On linux you can also use hostname -I \u2013 in general there can be many ways to find the same information using the command line, with different tradeoffs. This cheatsheet (or others, just google for 'bash cheat sheet') may come in handy: https://gist.github.com/LeCoupa/122b12050f5fb267e75f","title":"The Shell"},{"location":"resources/#ssh-and-scp","text":"https://www.youtube.com/watch?v=rm6pewTcSro","title":"SSH and SCP:"},{"location":"resources/#git","text":"https://www.youtube.com/watch?v=zbKdDsNNOhg (answers to \u201cwhy use Git in the first place?\u201d) https://www.youtube.com/watch?v=3a2x1iJFJWc https://youtu.be/9pa_PV2LUlw","title":"Git"},{"location":"resources/#writing-python-code","text":"You\u2019ll want to pick a code editor. Visual Studio Code, Sublime, Atom, GEdit, vim, and emacs are all popular choices. https://code.visualstudio.com/ is easy to install and configure, available on all platforms, and free.","title":"Writing Python code"},{"location":"resources/#ros-robot-operating-system","text":"One-hour introduction to ROS: https://www.youtube.com/watch?v=0BxVPCInS3M","title":"ROS (Robot Operating System)"}]}