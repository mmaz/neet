{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MIT NEET Labs: Spring 2019 FYI These labs are in-progress! Imitation Learning Lab Reinforcement Learning Lab","title":"MIT NEET Labs: Spring 2019"},{"location":"#mit-neet-labs-spring-2019","text":"FYI These labs are in-progress! Imitation Learning Lab Reinforcement Learning Lab","title":"MIT NEET Labs: Spring 2019"},{"location":"imitation_learning/","text":"Imitation Learning Lab Introduction This lab provides an introduction to end-to-end imitation learning for vision-only navigation of a racetrack. Let's break that down: We will train a deep learning model - specifically, a convolutional neural network (CNN) - to predict steering angles from images. Here, \"imitation learning\" refers to a branch of machine learning which focuses on imitating behavior from human-provided examples. In our case, we will drive a car around a track several times to provide examples for the CNN to mimic. We will contrast this with our next lab on \"reinforcement learning\" where a robot agent learns to accomplish a goal via exploration , not via examples. \"Vision-only\" refers to using an RGB camera as the only input to the machine learning algorithm. LIDAR, depth, or vehicle IMU data are not used. Here, \"end-to-end learning\" is shorthand for the CNN's ability to regress a steering angle (i.e., an actuation for the Ackermann controller) directly from unprocessed input data (pixels). We will not need to pre-process input features ourselves, such as extracting corners, walls, floors, or optical flow data. The CNN will learn which features are important, and perform all the steps from image processing to control estimation itself (\"end-to-end\", loosely speaking). We will start by driving a simulated car around a virtual racetrack and collecting camera data from the rendered game engine, as well as our game inputs. We will define a CNN that will predict similar game inputs in order for the car to complete the same track autonomously. Next, we will train the model using camera data and steering angles collected from the RACECAR platform in a real-world environment, the basement in Stata Center, in order for the RACECAR to autonomously drive through the Stata basement. In simulation: Lake Track Jungle Track In Stata basement: This lab and the CNN architecture we will use are based on PilotNet from Nvidia: Nvidia's blog post introducing the concept and their results Nvidia's PilotNet paper Udacity's Unity3D-based Self-Driving-Car Simulator and Naoki Shibuya's drive.py contributions Part 1: Install required Python libraries and the simulation environment TensorFlow, a deep-learning framework You first need to install miniconda to install TensorFlow. Download the Python 3.7 version of miniconda and follow the installation instructions for your platform. Warning We will be using Python 3.7 to define and train the PilotNet CNN model. Once we save a trained model (also known as saving weights ), we can later import the saved model in a Python 2 ROS environment on the RACECAR. TBD: Links to the environment files (this can be skipped on the server), and potentially switch to rapids/pytorch # Use TensorFlow without a GPU conda env create -f environment.yml # Use TensorFlow with a GPU conda env create -f environment-gpu.yml Udacity self-driving-car simulator Download the Udacity Term 1 simulator for your platform: Linux Mac Windows The full Unity3D source code of this simulator is available here , as well as other simulators for LIDAR data, waypoint-following, traffic, etc. We will only be using the simulator linked above. Extract the simulator (which will create a folder called beta_simulator_linux/ ): $ unzip term1-simulator-linux.zip On Linux, you will need to make the simulator executable, via the chmod command: $ chmod +x ./beta_simulator_linux/beta_simulator.x86_64 Part 2: Defining the PilotNet model Let us take a closer look at the CNN architecture for PilotNet: In this lab, we will command a fixed driving velocity and only regress steering angles from images using PilotNet. Hence, the PilotNet CNN has a single output. Using TensorFlow's Keras API , let us look at an implementation of the above network in code: from tensorflow.keras.layers import Lambda, Conv2D, MaxPooling2D, Dropout, Dense, Flatten from tensorflow.keras.models import Sequential # you will need to crop or shrink images to the dimensions you choose here: IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS = 66, 200, 3 INPUT_SHAPE = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS) def build_model(dropout_rate=0.5): model = Sequential() model.add(Lambda(lambda x: x/127.5-1.0, input_shape=INPUT_SHAPE)) #normalizes image data model.add(Conv2D(24, (5,5), strides=(2, 2), activation='elu')) model.add(Conv2D(36, (5,5), strides=(2, 2), activation='elu')) model.add(Conv2D(48, (5,5), strides=(2, 2), activation='elu')) model.add(Conv2D(64, (3,3), activation='elu')) model.add(Conv2D(64, (3,3), activation='elu')) model.add(Dropout(dropout_rate)) model.add(Flatten()) model.add(Dense(100, activation='elu')) model.add(Dense(50, activation='elu')) model.add(Dense(10, activation='elu')) model.add(Dense(1)) model.summary() return model As configured above, the PilotNet model expects 200x66 crops from the car's camera. Exercise How many parameters does each layer represent? What is the effect of changing the input size on the total number of parameters in the network? Why? Hint: use model.summary() as a way to explore the effect of changing input size. For more on TensorFlow's Keras API, click here . Note Note that Keras will disable Dropout regularization at inference time. See here for details. Model Output and Optimization The output of this model is a single neuron, which corresponds to the servo or steering angle to command the car with. In the section on Training we will normalize the output angles to fit between (-1, 1) ( Question: Why would we prefer to normalize the data?) We will use the Adam optimizer with a loss function (i.e., cost or objective function) that minimizes the mean square error betwen the ground-truth steering angles and the currently predicted steering angles: model = build_model() model.compile(loss='mean_squared_error', optimizer=Adam(lr=1.0e-4)) Optional Exercise ith only a few changes to the above model and loss definitions, you can add a second output to estimate the velocity as well. Part 3: Training the Model We will use three cameras mounted on the simulated car and the real-world RACECAR to collect training data. This excerpt from Nvidia's blog post explains why doing so is useful: Training data contains single images sampled from the video, paired with the corresponding steering command (1/r). Training with data from only the human driver is not sufficient; the network must also learn how to recover from any mistakes, or the car will slowly drift off the road. The training data is therefore augmented with additional images that show the car in different shifts from the center of the lane and rotations from the direction of the road. The images for two specific off-center shifts can be obtained from the left and the right cameras. Additional shifts between the cameras and all rotations are simulated through viewpoint transformation of the image from the nearest camera. Precise viewpoint transformation requires 3D scene knowledge which we don\u2019t have, so we approximate the transformation by assuming all points below the horizon are on flat ground, and all points above the horizon are infinitely far away. This works fine for flat terrain Here is a diagram from Nvidia that describes the training and data augmentation process for PilotNet: In Simulation First, create a new folder to store training data from the simulator, e.g. training/ ) and then start the simulator. On linux: $ ./beta_simulator_linux/beta_simulator.x86_64 Now launch the Training mode and configure the simulator to save data to the folder you created: Once you have configured a folder to record your training data into, press record again (or r as a shortcut) and start to drive the car around (you can use WASD or your arrow keys on your keyboard). If this is your first time running the simulator, stop recording after a few seconds, to inspect the saved results. The simulator will save three camera views from the car: left, center, and right camera views from the bumper of the car (as jpgs) along with the image filenames and the current steering angle in driving_log.csv . Warning Recording will generate a lot of files! Too many files in a single directory can cause performance issues, e.g., when generating thumbnails. Once you are ready to record full laps of the course, I recommend keeping each recording session to a few laps of the track, and making multiple new folders. This will help to keep the number of files within each folder low, e.g., two_laps_run1/ , two_laps_run2/ , etc, or making a folder for tricky sections of the course, e.g., bridge_section_run1/ . It is easy to concatenate the resulting CSVs in python (using simple list concatenation with + ) In the training/ folder (or whichever folder you just created), you should see driving_log.csv and another folder IMG/ . driving_log.csv contains path locations for the three camera views with associated timestamps (sequentially increasing), and the saved steering angle (without a CSV header): Center Left Right Steering Angle Throttle Brake Speed center_2019_03_11_12_22_15_385.jpg left_2019_03_11_12_22_15_385.jpg right_2019_03_11_12_22_15_385.jpg 0 0 0 0 center_2019_03_11_12_22_15_502.jpg left_2019_03_11_12_22_15_502.jpg right_2019_03_11_12_22_15_502.jpg 0 0 0 0 center_2019_03_11_12_22_15_594.jpg left_2019_03_11_12_22_15_594.jpg right_2019_03_11_12_22_15_594.jpg 0 0 0 0 Note If you would like to change some of the parameters, such as the saved image resolution, or even define a new track, you can rebuild and edit the simulator using Unity3D . See this section of the Udacity source code if you are curious how the image files and CSV data are generated. In the training/IMG/ folder you will find .jpg files with the following naming scheme corresponding to the above CSV: Left Center Right center_2019_03_11_12_22_15_385.jpg left_2019_03_11_12_22_15_385.jpg right_2019_03_11_12_22_15_385.jpg Train/Validation Split Danger With enough training time and enough parameters, you can perfectly fit your training data! Therefore, you will need to partition your data into training and validation data. Validation helps to ensure your model is not overfitting on the training data. imgs = [] angles_rad = [] #normalize between -pi to pi or -1 to 1 with open(driving_data) as fh: ########################## # ### TODO: read in the CSV # ### and fill in the above # ### lists # ############################ TEST_SIZE_FRACTION = 0.2 SEED = 56709 # a fixed seed can be convenient for later comparisons X_train, X_valid, y_train, y_valid = train_test_split( imgs, angles_rad, test_size=TEST_SIZE_FRACTION, random_state=SEED) Batch Generation, Checkpointing, and Training Execution For efficient training on a GPU, multiple examples are sent at once in a batch onto the GPU in a single copy operation, and the results of backpropagation are returned from the GPU back to the CPU. You will want to checkpoint your model after each epoch of training. Lastly, model.fit_generator() will commence training on your data and display the current loss on your training and testing data: checkpoint = ModelCheckpoint('imitationlearning-{epoch:03d}.h5', monitor='val_loss', verbose=0, save_best_only=False, mode='auto') def batch_generator(image_paths, steering_angles, batch_size): \"\"\" Generate training image give image paths and associated steering angles \"\"\" images = np.empty([batch_size, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS]) steers = np.empty(batch_size) while True: i = 0 for index in np.random.permutation(len(image_paths)): ############################################## # TODO: add your augmentation code here! #### # NOTE: you may want to disable #### # augmentation when validating! #### ############################################## image = cv.imread(image_paths[index]) cropped = image[95:-95, 128:-127, :] images[i] = cropped steering_angle = steering_angles[index] steers[i] = steering_angle i += 1 if i == batch_size: break yield images, steers BATCH_SIZE=20 model.fit_generator(generator=batch_generator(X_train, y_train, batch_size=BATCH_SIZE), steps_per_epoch=20000, epochs=10, validation_data=batch_generator(X_valid, y_valid, batch_size=BATCH_SIZE), # https://stackoverflow.com/a/45944225 validation_steps=len(X_valid) // BATCH_SIZE, callbacks=[checkpoint], verbose=1) Image Augmentation You will want to add some data augmentation to help your model generalize past the specific examples you have collected in the simulator (and on the actual RACECAR). Some example transformations to incorporate: Center Image Left and right Images def choose_image(data_dir, center, left, right, steering_angle): \"\"\" Randomly choose an image from the center, left or right, and adjust the steering angle. \"\"\" choice = np.random.choice(3) if choice == 0: return load_image(data_dir, left), steering_angle + 0.2 elif choice == 1: return load_image(data_dir, right), steering_angle - 0.2 return load_image(data_dir, center), steering_angle Flipped Image if np.random.rand() < 0.5: image = cv2.flip(image, 1) steering_angle = -steering_angle return image, steering_angle Translated Image def random_translate(image, steering_angle, range_x, range_y): \"\"\" Randomly shift the image virtially and horizontally (translation). \"\"\" trans_x = range_x * (np.random.rand() - 0.5) trans_y = range_y * (np.random.rand() - 0.5) steering_angle += trans_x * 0.002 trans_m = np.float32([[1, 0, trans_x], [0, 1, trans_y]]) height, width = image.shape[:2] image = cv2.warpAffine(image, trans_m, (width, height)) return image, steering_angle Servo histograms It is important to ensure the train/test split of the data you have collected have similar driving condition represented. For instance, here is the histogram of servo angles in the training and testing data used above: Checkpointing checkpoint = ModelCheckpoint('markmodel2-{epoch:03d}.h5', monitor='val_loss', verbose=0, save_best_only=False, mode='auto') [Optional] Extending to more general environments It is possible to train a network with driving data from public roads, in order to experiment with how it affects the performance of your car in Stata basement. Danger Obviously, you should not test anything on public roads yourself, either on a RACECAR or any other car. Be safe and responsible! Part 4: RACECAR data collection and training You will need to save images to the car's SSD: In zed.launch ( $ roscd zed_wrapper ): <arg name=\"resolution\" default=\"3\" /> <!--0=RESOLUTION_HD2K, 1=RESOLUTION_HD1080, 2=RESOLUTION_HD720, 3=RESOLUTION_VGA --> <arg name=\"frame_rate\" default=\"15\" /> In TODO launch/record_bag.launch : args=\"--output-prefix $(arg saveas) $(arg extra_args) /joy /racecar_drive /vesc/sensors/core /velodyne_packets /scan /imu/data_raw /imu/data /imu/mag /zed/left/image_raw_color/compressed /zed/right/image_raw_color/compressed /zed/left/camera_info /zed/right/camera_info\" /> Then, to record the rosbag TODO : $ roslaunch racecar_bringup record_bag.launch saveas:=/media/ssd/rosbags/ Part 5: Running inference on RACECAR The following script will load a pre-trained model and drive the car through Stata basement: $ # Optional -- mount SSD for data collection $ # sudo mount -t auto /dev/sda1 /media/ssd/ $ roslaunch racecar_bringup base.launch teleop:=true $ roslaunch zed_wrapper zed.launch $ cd ~/pilotnet # or wherever you have basement-006.h5 weights stored $ python pilotnet_drive.py","title":"Imitation Learning Lab"},{"location":"imitation_learning/#imitation-learning-lab","text":"","title":"Imitation Learning Lab"},{"location":"imitation_learning/#introduction","text":"This lab provides an introduction to end-to-end imitation learning for vision-only navigation of a racetrack. Let's break that down: We will train a deep learning model - specifically, a convolutional neural network (CNN) - to predict steering angles from images. Here, \"imitation learning\" refers to a branch of machine learning which focuses on imitating behavior from human-provided examples. In our case, we will drive a car around a track several times to provide examples for the CNN to mimic. We will contrast this with our next lab on \"reinforcement learning\" where a robot agent learns to accomplish a goal via exploration , not via examples. \"Vision-only\" refers to using an RGB camera as the only input to the machine learning algorithm. LIDAR, depth, or vehicle IMU data are not used. Here, \"end-to-end learning\" is shorthand for the CNN's ability to regress a steering angle (i.e., an actuation for the Ackermann controller) directly from unprocessed input data (pixels). We will not need to pre-process input features ourselves, such as extracting corners, walls, floors, or optical flow data. The CNN will learn which features are important, and perform all the steps from image processing to control estimation itself (\"end-to-end\", loosely speaking). We will start by driving a simulated car around a virtual racetrack and collecting camera data from the rendered game engine, as well as our game inputs. We will define a CNN that will predict similar game inputs in order for the car to complete the same track autonomously. Next, we will train the model using camera data and steering angles collected from the RACECAR platform in a real-world environment, the basement in Stata Center, in order for the RACECAR to autonomously drive through the Stata basement.","title":"Introduction"},{"location":"imitation_learning/#in-simulation","text":"Lake Track Jungle Track","title":"In simulation:"},{"location":"imitation_learning/#in-stata-basement","text":"This lab and the CNN architecture we will use are based on PilotNet from Nvidia: Nvidia's blog post introducing the concept and their results Nvidia's PilotNet paper Udacity's Unity3D-based Self-Driving-Car Simulator and Naoki Shibuya's drive.py contributions","title":"In Stata basement:"},{"location":"imitation_learning/#part-1-install-required-python-libraries-and-the-simulation-environment","text":"","title":"Part 1: Install required Python libraries and the simulation environment"},{"location":"imitation_learning/#tensorflow-a-deep-learning-framework","text":"You first need to install miniconda to install TensorFlow. Download the Python 3.7 version of miniconda and follow the installation instructions for your platform. Warning We will be using Python 3.7 to define and train the PilotNet CNN model. Once we save a trained model (also known as saving weights ), we can later import the saved model in a Python 2 ROS environment on the RACECAR. TBD: Links to the environment files (this can be skipped on the server), and potentially switch to rapids/pytorch # Use TensorFlow without a GPU conda env create -f environment.yml # Use TensorFlow with a GPU conda env create -f environment-gpu.yml","title":"TensorFlow, a deep-learning framework"},{"location":"imitation_learning/#udacity-self-driving-car-simulator","text":"Download the Udacity Term 1 simulator for your platform: Linux Mac Windows The full Unity3D source code of this simulator is available here , as well as other simulators for LIDAR data, waypoint-following, traffic, etc. We will only be using the simulator linked above. Extract the simulator (which will create a folder called beta_simulator_linux/ ): $ unzip term1-simulator-linux.zip On Linux, you will need to make the simulator executable, via the chmod command: $ chmod +x ./beta_simulator_linux/beta_simulator.x86_64","title":"Udacity self-driving-car simulator"},{"location":"imitation_learning/#part-2-defining-the-pilotnet-model","text":"Let us take a closer look at the CNN architecture for PilotNet: In this lab, we will command a fixed driving velocity and only regress steering angles from images using PilotNet. Hence, the PilotNet CNN has a single output. Using TensorFlow's Keras API , let us look at an implementation of the above network in code: from tensorflow.keras.layers import Lambda, Conv2D, MaxPooling2D, Dropout, Dense, Flatten from tensorflow.keras.models import Sequential # you will need to crop or shrink images to the dimensions you choose here: IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS = 66, 200, 3 INPUT_SHAPE = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS) def build_model(dropout_rate=0.5): model = Sequential() model.add(Lambda(lambda x: x/127.5-1.0, input_shape=INPUT_SHAPE)) #normalizes image data model.add(Conv2D(24, (5,5), strides=(2, 2), activation='elu')) model.add(Conv2D(36, (5,5), strides=(2, 2), activation='elu')) model.add(Conv2D(48, (5,5), strides=(2, 2), activation='elu')) model.add(Conv2D(64, (3,3), activation='elu')) model.add(Conv2D(64, (3,3), activation='elu')) model.add(Dropout(dropout_rate)) model.add(Flatten()) model.add(Dense(100, activation='elu')) model.add(Dense(50, activation='elu')) model.add(Dense(10, activation='elu')) model.add(Dense(1)) model.summary() return model As configured above, the PilotNet model expects 200x66 crops from the car's camera. Exercise How many parameters does each layer represent? What is the effect of changing the input size on the total number of parameters in the network? Why? Hint: use model.summary() as a way to explore the effect of changing input size. For more on TensorFlow's Keras API, click here . Note Note that Keras will disable Dropout regularization at inference time. See here for details.","title":"Part 2: Defining the PilotNet model"},{"location":"imitation_learning/#model-output-and-optimization","text":"The output of this model is a single neuron, which corresponds to the servo or steering angle to command the car with. In the section on Training we will normalize the output angles to fit between (-1, 1) ( Question: Why would we prefer to normalize the data?) We will use the Adam optimizer with a loss function (i.e., cost or objective function) that minimizes the mean square error betwen the ground-truth steering angles and the currently predicted steering angles: model = build_model() model.compile(loss='mean_squared_error', optimizer=Adam(lr=1.0e-4)) Optional Exercise ith only a few changes to the above model and loss definitions, you can add a second output to estimate the velocity as well.","title":"Model Output and Optimization"},{"location":"imitation_learning/#part-3-training-the-model","text":"We will use three cameras mounted on the simulated car and the real-world RACECAR to collect training data. This excerpt from Nvidia's blog post explains why doing so is useful: Training data contains single images sampled from the video, paired with the corresponding steering command (1/r). Training with data from only the human driver is not sufficient; the network must also learn how to recover from any mistakes, or the car will slowly drift off the road. The training data is therefore augmented with additional images that show the car in different shifts from the center of the lane and rotations from the direction of the road. The images for two specific off-center shifts can be obtained from the left and the right cameras. Additional shifts between the cameras and all rotations are simulated through viewpoint transformation of the image from the nearest camera. Precise viewpoint transformation requires 3D scene knowledge which we don\u2019t have, so we approximate the transformation by assuming all points below the horizon are on flat ground, and all points above the horizon are infinitely far away. This works fine for flat terrain Here is a diagram from Nvidia that describes the training and data augmentation process for PilotNet:","title":"Part 3: Training the Model"},{"location":"imitation_learning/#in-simulation_1","text":"First, create a new folder to store training data from the simulator, e.g. training/ ) and then start the simulator. On linux: $ ./beta_simulator_linux/beta_simulator.x86_64 Now launch the Training mode and configure the simulator to save data to the folder you created: Once you have configured a folder to record your training data into, press record again (or r as a shortcut) and start to drive the car around (you can use WASD or your arrow keys on your keyboard). If this is your first time running the simulator, stop recording after a few seconds, to inspect the saved results. The simulator will save three camera views from the car: left, center, and right camera views from the bumper of the car (as jpgs) along with the image filenames and the current steering angle in driving_log.csv . Warning Recording will generate a lot of files! Too many files in a single directory can cause performance issues, e.g., when generating thumbnails. Once you are ready to record full laps of the course, I recommend keeping each recording session to a few laps of the track, and making multiple new folders. This will help to keep the number of files within each folder low, e.g., two_laps_run1/ , two_laps_run2/ , etc, or making a folder for tricky sections of the course, e.g., bridge_section_run1/ . It is easy to concatenate the resulting CSVs in python (using simple list concatenation with + ) In the training/ folder (or whichever folder you just created), you should see driving_log.csv and another folder IMG/ . driving_log.csv contains path locations for the three camera views with associated timestamps (sequentially increasing), and the saved steering angle (without a CSV header): Center Left Right Steering Angle Throttle Brake Speed center_2019_03_11_12_22_15_385.jpg left_2019_03_11_12_22_15_385.jpg right_2019_03_11_12_22_15_385.jpg 0 0 0 0 center_2019_03_11_12_22_15_502.jpg left_2019_03_11_12_22_15_502.jpg right_2019_03_11_12_22_15_502.jpg 0 0 0 0 center_2019_03_11_12_22_15_594.jpg left_2019_03_11_12_22_15_594.jpg right_2019_03_11_12_22_15_594.jpg 0 0 0 0 Note If you would like to change some of the parameters, such as the saved image resolution, or even define a new track, you can rebuild and edit the simulator using Unity3D . See this section of the Udacity source code if you are curious how the image files and CSV data are generated. In the training/IMG/ folder you will find .jpg files with the following naming scheme corresponding to the above CSV: Left Center Right center_2019_03_11_12_22_15_385.jpg left_2019_03_11_12_22_15_385.jpg right_2019_03_11_12_22_15_385.jpg","title":"In Simulation"},{"location":"imitation_learning/#trainvalidation-split","text":"Danger With enough training time and enough parameters, you can perfectly fit your training data! Therefore, you will need to partition your data into training and validation data. Validation helps to ensure your model is not overfitting on the training data. imgs = [] angles_rad = [] #normalize between -pi to pi or -1 to 1 with open(driving_data) as fh: ########################## # ### TODO: read in the CSV # ### and fill in the above # ### lists # ############################ TEST_SIZE_FRACTION = 0.2 SEED = 56709 # a fixed seed can be convenient for later comparisons X_train, X_valid, y_train, y_valid = train_test_split( imgs, angles_rad, test_size=TEST_SIZE_FRACTION, random_state=SEED)","title":"Train/Validation Split"},{"location":"imitation_learning/#batch-generation-checkpointing-and-training-execution","text":"For efficient training on a GPU, multiple examples are sent at once in a batch onto the GPU in a single copy operation, and the results of backpropagation are returned from the GPU back to the CPU. You will want to checkpoint your model after each epoch of training. Lastly, model.fit_generator() will commence training on your data and display the current loss on your training and testing data: checkpoint = ModelCheckpoint('imitationlearning-{epoch:03d}.h5', monitor='val_loss', verbose=0, save_best_only=False, mode='auto') def batch_generator(image_paths, steering_angles, batch_size): \"\"\" Generate training image give image paths and associated steering angles \"\"\" images = np.empty([batch_size, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS]) steers = np.empty(batch_size) while True: i = 0 for index in np.random.permutation(len(image_paths)): ############################################## # TODO: add your augmentation code here! #### # NOTE: you may want to disable #### # augmentation when validating! #### ############################################## image = cv.imread(image_paths[index]) cropped = image[95:-95, 128:-127, :] images[i] = cropped steering_angle = steering_angles[index] steers[i] = steering_angle i += 1 if i == batch_size: break yield images, steers BATCH_SIZE=20 model.fit_generator(generator=batch_generator(X_train, y_train, batch_size=BATCH_SIZE), steps_per_epoch=20000, epochs=10, validation_data=batch_generator(X_valid, y_valid, batch_size=BATCH_SIZE), # https://stackoverflow.com/a/45944225 validation_steps=len(X_valid) // BATCH_SIZE, callbacks=[checkpoint], verbose=1)","title":"Batch Generation, Checkpointing, and Training Execution"},{"location":"imitation_learning/#image-augmentation","text":"You will want to add some data augmentation to help your model generalize past the specific examples you have collected in the simulator (and on the actual RACECAR). Some example transformations to incorporate: Center Image Left and right Images def choose_image(data_dir, center, left, right, steering_angle): \"\"\" Randomly choose an image from the center, left or right, and adjust the steering angle. \"\"\" choice = np.random.choice(3) if choice == 0: return load_image(data_dir, left), steering_angle + 0.2 elif choice == 1: return load_image(data_dir, right), steering_angle - 0.2 return load_image(data_dir, center), steering_angle Flipped Image if np.random.rand() < 0.5: image = cv2.flip(image, 1) steering_angle = -steering_angle return image, steering_angle Translated Image def random_translate(image, steering_angle, range_x, range_y): \"\"\" Randomly shift the image virtially and horizontally (translation). \"\"\" trans_x = range_x * (np.random.rand() - 0.5) trans_y = range_y * (np.random.rand() - 0.5) steering_angle += trans_x * 0.002 trans_m = np.float32([[1, 0, trans_x], [0, 1, trans_y]]) height, width = image.shape[:2] image = cv2.warpAffine(image, trans_m, (width, height)) return image, steering_angle","title":"Image Augmentation"},{"location":"imitation_learning/#servo-histograms","text":"It is important to ensure the train/test split of the data you have collected have similar driving condition represented. For instance, here is the histogram of servo angles in the training and testing data used above:","title":"Servo histograms"},{"location":"imitation_learning/#checkpointing","text":"checkpoint = ModelCheckpoint('markmodel2-{epoch:03d}.h5', monitor='val_loss', verbose=0, save_best_only=False, mode='auto')","title":"Checkpointing"},{"location":"imitation_learning/#optional-extending-to-more-general-environments","text":"It is possible to train a network with driving data from public roads, in order to experiment with how it affects the performance of your car in Stata basement. Danger Obviously, you should not test anything on public roads yourself, either on a RACECAR or any other car. Be safe and responsible!","title":"[Optional] Extending to more general environments"},{"location":"imitation_learning/#part-4-racecar-data-collection-and-training","text":"You will need to save images to the car's SSD: In zed.launch ( $ roscd zed_wrapper ): <arg name=\"resolution\" default=\"3\" /> <!--0=RESOLUTION_HD2K, 1=RESOLUTION_HD1080, 2=RESOLUTION_HD720, 3=RESOLUTION_VGA --> <arg name=\"frame_rate\" default=\"15\" /> In TODO launch/record_bag.launch : args=\"--output-prefix $(arg saveas) $(arg extra_args) /joy /racecar_drive /vesc/sensors/core /velodyne_packets /scan /imu/data_raw /imu/data /imu/mag /zed/left/image_raw_color/compressed /zed/right/image_raw_color/compressed /zed/left/camera_info /zed/right/camera_info\" /> Then, to record the rosbag TODO : $ roslaunch racecar_bringup record_bag.launch saveas:=/media/ssd/rosbags/","title":"Part 4: RACECAR data collection and training"},{"location":"imitation_learning/#part-5-running-inference-on-racecar","text":"The following script will load a pre-trained model and drive the car through Stata basement: $ # Optional -- mount SSD for data collection $ # sudo mount -t auto /dev/sda1 /media/ssd/ $ roslaunch racecar_bringup base.launch teleop:=true $ roslaunch zed_wrapper zed.launch $ cd ~/pilotnet # or wherever you have basement-006.h5 weights stored $ python pilotnet_drive.py","title":"Part 5: Running inference on RACECAR"},{"location":"neet_fall_2018/","text":"MIT NEET Fall 2018 Multiplex SSH This is highly recommended for connecting to the drones: Create or edit a file in the .ssh/ directory inside your home directory, called config : $ nano ~/.ssh/config or $ gedit ~/.ssh/config or $ vi ~/.ssh/config (or whichever text editor you are comfortable with) You will need to add two lines to the file, and you will need to replace PATH_TO_HOME with the correct path for your platform: on linux, /home/YOURTEAMNAME/.ssh/ (or your username, if you are not using a team laptop) on mac, /Users/YOURUSERNAME/.ssh windows, (TODO) Add the following two lines ( Remember: change PATH_TO_HOME as specified above): ControlMaster auto ControlPath PATH_TO_HOME/.ssh/ssh_mux_%h_%p_%r Important - remember to start your FIRST SSH connection with -Y if you plan to use xforwarding (e.g., for rqt_image_view ) Directory Setup These instructions replace this section on the website. On the SSH window on your team laptop, enter the following commands cd ~ cd ~/bwsi-uav/catkin_ws/src git clone https://github.com/BWSI-UAV/aero_control.git cd aero_control git remote add upstream https://github.com/BWSI-UAV/aero_control.git cd ~/bwsi-uav git clone https://github.com/BWSI-UAV/laboratory.git cd laboratory git remote add upstream https://github.com/BWSI-UAV/laboratory.git cd ~/bwsi-uav git clone https://github.com/BWSI-UAV/documents.git cd documents git remote add upstream https://github.com/BWSI-UAV/documents.git Compressed Camera Feeds To find if your drone supports compressed camera feeds: Start roscore Start optical flow: sudo -E ~/bwsi-uav/catkin-ws/src/aero-optical-flow/build/aero-optical-flow $ rostopic list | grep compressed If you don't see /aero_downward_camera/image/compressed in the results you will need to install compressed transport support: sudo apt-get install ros-kinetic-image-transport-plugins then restart your camera feed by restarting the aero-optical-flow binary (step 1 above). To record a compressed downward camera feed: $ cd ~/rosbags/ # or wherever you want to store your rosbag $ time rosbag record -O downward /aero_downward_camera/image/compressed # -O specifies the filename You can then SCP your rosbag to your team laptop. To convert compressed camera messages to OpenCV images, you can't use CVBridge. Here is an OpenCV-specific decoding solution. (You could also use CompressedImage from sensor_msgs.msg ): from __future__ import print_function import cv2 import numpy as np import roslib import rospy from sensor_msgs.msg import CompressedImage # We do not use cv_bridge since it does not support CompressedImage # from cv_bridge import CvBridge, CvBridgeError import rosbag import os DEST = \"/path/to/folder/to/save/images\" BAG = \"/path/to/rosbag.bag\" #your camera topic: CAM = '/aero_downward_camera/image/compressed' def bag2msgs(): bag = rosbag.Bag(BAG) if bag is None: raise ValueError(\"no bag {}\".format(BAG)) msgs = [] for topic, msg, t in bag.read_messages(topics=[CAM]): msgs.append(msg) bag.close() print(\"MESSAGES: {}\".format(len(msgs))) return msgs def uncompress(msgs): imgs = [] for msg in msgs: #### direct conversion to CV2 #### np_arr = np.fromstring(msg.data, np.uint8) image_np = cv2.imdecode(np_arr, cv2.IMREAD_COLOR) # OpenCV >= 3.0: imgs.append(image_np) return imgs if __name__ == \"__main__\": if os.listdir(DEST) != []: raise ValueError('need empty directory for dest {}'.format(DEST)) msgs = bag2msgs() imgs = uncompress(msgs) for idx,im in enumerate(imgs): if idx % 50 == 0: print(idx) imname = \"frame{:05d}.jpg\".format(idx) cv2.imwrite(DEST + imname, im)","title":"MIT NEET Fall 2018"},{"location":"neet_fall_2018/#mit-neet-fall-2018","text":"","title":"MIT NEET Fall 2018"},{"location":"neet_fall_2018/#multiplex-ssh","text":"This is highly recommended for connecting to the drones: Create or edit a file in the .ssh/ directory inside your home directory, called config : $ nano ~/.ssh/config or $ gedit ~/.ssh/config or $ vi ~/.ssh/config (or whichever text editor you are comfortable with) You will need to add two lines to the file, and you will need to replace PATH_TO_HOME with the correct path for your platform: on linux, /home/YOURTEAMNAME/.ssh/ (or your username, if you are not using a team laptop) on mac, /Users/YOURUSERNAME/.ssh windows, (TODO) Add the following two lines ( Remember: change PATH_TO_HOME as specified above): ControlMaster auto ControlPath PATH_TO_HOME/.ssh/ssh_mux_%h_%p_%r Important - remember to start your FIRST SSH connection with -Y if you plan to use xforwarding (e.g., for rqt_image_view )","title":"Multiplex SSH"},{"location":"neet_fall_2018/#directory-setup","text":"These instructions replace this section on the website. On the SSH window on your team laptop, enter the following commands cd ~ cd ~/bwsi-uav/catkin_ws/src git clone https://github.com/BWSI-UAV/aero_control.git cd aero_control git remote add upstream https://github.com/BWSI-UAV/aero_control.git cd ~/bwsi-uav git clone https://github.com/BWSI-UAV/laboratory.git cd laboratory git remote add upstream https://github.com/BWSI-UAV/laboratory.git cd ~/bwsi-uav git clone https://github.com/BWSI-UAV/documents.git cd documents git remote add upstream https://github.com/BWSI-UAV/documents.git","title":"Directory Setup"},{"location":"neet_fall_2018/#compressed-camera-feeds","text":"To find if your drone supports compressed camera feeds: Start roscore Start optical flow: sudo -E ~/bwsi-uav/catkin-ws/src/aero-optical-flow/build/aero-optical-flow $ rostopic list | grep compressed If you don't see /aero_downward_camera/image/compressed in the results you will need to install compressed transport support: sudo apt-get install ros-kinetic-image-transport-plugins then restart your camera feed by restarting the aero-optical-flow binary (step 1 above). To record a compressed downward camera feed: $ cd ~/rosbags/ # or wherever you want to store your rosbag $ time rosbag record -O downward /aero_downward_camera/image/compressed # -O specifies the filename You can then SCP your rosbag to your team laptop. To convert compressed camera messages to OpenCV images, you can't use CVBridge. Here is an OpenCV-specific decoding solution. (You could also use CompressedImage from sensor_msgs.msg ): from __future__ import print_function import cv2 import numpy as np import roslib import rospy from sensor_msgs.msg import CompressedImage # We do not use cv_bridge since it does not support CompressedImage # from cv_bridge import CvBridge, CvBridgeError import rosbag import os DEST = \"/path/to/folder/to/save/images\" BAG = \"/path/to/rosbag.bag\" #your camera topic: CAM = '/aero_downward_camera/image/compressed' def bag2msgs(): bag = rosbag.Bag(BAG) if bag is None: raise ValueError(\"no bag {}\".format(BAG)) msgs = [] for topic, msg, t in bag.read_messages(topics=[CAM]): msgs.append(msg) bag.close() print(\"MESSAGES: {}\".format(len(msgs))) return msgs def uncompress(msgs): imgs = [] for msg in msgs: #### direct conversion to CV2 #### np_arr = np.fromstring(msg.data, np.uint8) image_np = cv2.imdecode(np_arr, cv2.IMREAD_COLOR) # OpenCV >= 3.0: imgs.append(image_np) return imgs if __name__ == \"__main__\": if os.listdir(DEST) != []: raise ValueError('need empty directory for dest {}'.format(DEST)) msgs = bag2msgs() imgs = uncompress(msgs) for idx,im in enumerate(imgs): if idx % 50 == 0: print(idx) imname = \"frame{:05d}.jpg\".format(idx) cv2.imwrite(DEST + imname, im)","title":"Compressed Camera Feeds"},{"location":"reinforcement_learning/","text":"Reinforcement Learning To be released soon!","title":"Reinforcement Learning"},{"location":"reinforcement_learning/#reinforcement-learning","text":"To be released soon!","title":"Reinforcement Learning"},{"location":"resources/","text":"Resources You can contribute to this list! Do you have tips, resources, or corrections to add? Did you find something frustrating or unhelpful and want to remove it? Notice any typos? Pull-requests welcome! Submit one at https://github.com/mmaz/neet UAV Course website: http://neet.beaver.works If this is your first time using a shell, git, or ssh, here are a few tutorials to help acquaint yourself with the basics. Tip: 2x speed videos On youtube, click the gear icon and change the playback speed to 2x, which is useful when watching slower-paced videos. Python MIT's own intro to python and programming course is a good place to review python basics, such as branching, iteration, and lists. Videos are also here: youtube playlist The Shell The shell (aka, command line or terminal) is how we connect to the UAV. Shell basics: https://www.youtube.com/watch?v=poT5Yd0Ag8I https://www.youtube.com/watch?v=oxuRxtrO2Ag https://www.digitalocean.com/community/tutorials/how-to-use-cd-pwd-and-ls-to-explore-the-file-system-on-a-linux-server Some useful commands to learn are: ls, cd, pwd, find, grep, mkdir, rm, mv, cp, less, cat, which Shell operators like | (the pipe symbol) and > as well as control signals, like Ctrl+C , are also useful to know. You may need to occasionally find your IP address and check for network reachability. Some useful commands for this: ping, ifconfig, wget, curl For example, $ ping drone.beaver.works I often find my IP address with the following command: ifconfig | grep inet How does this work? ifconfig spits out a lot of network configuration information. The | symbol pipes the output of ifconfig to grep , which searches for the word 'inet' on each line in the output. This corresponds to a list of IP addresses associated with my computer. On linux you can also use hostname -I \u2013 in general there can be many ways to find the same information using the command line, with different tradeoffs. This cheatsheet (or others, just google for 'bash cheat sheet') may come in handy: https://gist.github.com/LeCoupa/122b12050f5fb267e75f SSH and SCP: https://www.youtube.com/watch?v=rm6pewTcSro Git https://www.youtube.com/watch?v=zbKdDsNNOhg (answers to \u201cwhy use Git in the first place?\u201d) https://www.youtube.com/watch?v=3a2x1iJFJWc https://youtu.be/9pa_PV2LUlw Writing Python code You\u2019ll want to pick a code editor. Visual Studio Code, Sublime, Atom, GEdit, vim, and emacs are all popular choices. https://code.visualstudio.com/ is easy to install and configure, available on all platforms, and free. ROS (Robot Operating System) One-hour introduction to ROS: https://www.youtube.com/watch?v=0BxVPCInS3M","title":"Resources"},{"location":"resources/#resources","text":"You can contribute to this list! Do you have tips, resources, or corrections to add? Did you find something frustrating or unhelpful and want to remove it? Notice any typos? Pull-requests welcome! Submit one at https://github.com/mmaz/neet UAV Course website: http://neet.beaver.works If this is your first time using a shell, git, or ssh, here are a few tutorials to help acquaint yourself with the basics. Tip: 2x speed videos On youtube, click the gear icon and change the playback speed to 2x, which is useful when watching slower-paced videos.","title":"Resources"},{"location":"resources/#python","text":"MIT's own intro to python and programming course is a good place to review python basics, such as branching, iteration, and lists. Videos are also here: youtube playlist","title":"Python"},{"location":"resources/#the-shell","text":"The shell (aka, command line or terminal) is how we connect to the UAV. Shell basics: https://www.youtube.com/watch?v=poT5Yd0Ag8I https://www.youtube.com/watch?v=oxuRxtrO2Ag https://www.digitalocean.com/community/tutorials/how-to-use-cd-pwd-and-ls-to-explore-the-file-system-on-a-linux-server Some useful commands to learn are: ls, cd, pwd, find, grep, mkdir, rm, mv, cp, less, cat, which Shell operators like | (the pipe symbol) and > as well as control signals, like Ctrl+C , are also useful to know. You may need to occasionally find your IP address and check for network reachability. Some useful commands for this: ping, ifconfig, wget, curl For example, $ ping drone.beaver.works I often find my IP address with the following command: ifconfig | grep inet How does this work? ifconfig spits out a lot of network configuration information. The | symbol pipes the output of ifconfig to grep , which searches for the word 'inet' on each line in the output. This corresponds to a list of IP addresses associated with my computer. On linux you can also use hostname -I \u2013 in general there can be many ways to find the same information using the command line, with different tradeoffs. This cheatsheet (or others, just google for 'bash cheat sheet') may come in handy: https://gist.github.com/LeCoupa/122b12050f5fb267e75f","title":"The Shell"},{"location":"resources/#ssh-and-scp","text":"https://www.youtube.com/watch?v=rm6pewTcSro","title":"SSH and SCP:"},{"location":"resources/#git","text":"https://www.youtube.com/watch?v=zbKdDsNNOhg (answers to \u201cwhy use Git in the first place?\u201d) https://www.youtube.com/watch?v=3a2x1iJFJWc https://youtu.be/9pa_PV2LUlw","title":"Git"},{"location":"resources/#writing-python-code","text":"You\u2019ll want to pick a code editor. Visual Studio Code, Sublime, Atom, GEdit, vim, and emacs are all popular choices. https://code.visualstudio.com/ is easy to install and configure, available on all platforms, and free.","title":"Writing Python code"},{"location":"resources/#ros-robot-operating-system","text":"One-hour introduction to ROS: https://www.youtube.com/watch?v=0BxVPCInS3M","title":"ROS (Robot Operating System)"}]}